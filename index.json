[{"content":"This list is being updated on regular basis\nGeneral info  https://kubernetesreadme.com/  Comparison  API comparison https://kube-api.ninja/ https://learnk8s.io/research https://docs.google.com/spreadsheets/d/1RPpyDOLFmcgxMCpABDzrsBYWpPYCIBuvAoUQLwOGoQw/edit#gid=907731238  Hosting  kapsule https://www.scaleway.com/en/ free https://cloud.okteto.com/  Secrets  External secrets management integration with k8s https://github.com/godaddy/kubernetes-external-secrets Integrate Kubernetes with 1Password https://github.com/1Password/onepassword-operator Kubernetes mutating webhook for secrets-init injection https://github.com/doitintl/kube-secrets-init AWS EKS Secrets store CSI driver https://aws.amazon.com/blogs/security/how-to-use-aws-secrets-configuration-provider-with-kubernetes-secrets-store-csi-driver/  RBAC  RBAC practices and tooling https://rbac.dev/ Visualize RBAC https://github.com/team-soteria/rback RBAC Manager is designed to simplify authorization in Kubernetes https://github.com/FairwindsOps/rbac-manager Access matrix https://github.com/corneliusweig/rakkess  Security  Kubernetes vector attack https://github.com/cyberark/kubesploit  https://github.com/cyberark/kubesploit/blob/assets/mitre_pic_full.png    Production checklist  https://learnk8s.io/production-best-practices/  Deployment Helm  https://v3.helm.sh/docs/howto/charts_tips_and_tricks  Serverless  Knative https://knative.dev/docs/eventing/sources/ Kubeless https://kubeless.io/ OpenFAAS https://github.com/openfaas/faas  Local  Minikube https://minikube.sigs.k8s.io/ https://docs.tilt.dev/ k0s https://github.com/k0sproject/k0s Microk8s from Canonical https://microk8s.io/ KIND https://kind.sigs.k8s.io/ Telepresence https://www.telepresence.io/tutorials/kubernetes-rapid Exposes your local resources to kubernetes (like telepresence) https://github.com/omrikiei/ktunnel Skaffold https://skaffold.dev/  Capacity  Overview of the resource requests, limits, and utilization in a Kubernetes https://github.com/robscott/kube-capacity Recommendations requests/limits https://github.com/robusta-dev/krr  K8s Plugins  Plugin installer https://github.com/kubernetes-sigs/krew  Plugins list https://krew.sigs.k8s.io/plugins   Debug pods https://github.com/aylei/kubectl-debug Resources https://github.com/y0zg/kubectl-resources View webhook https://github.com/Trendyol/kubectl-view-webhook#kubectl-view-webhook access-matrix - show an access matrix for k8s server resources https://github.com/corneliusweig/rakkess rbac-lookup - Easily find roles and cluster roles attached to any user, service account, or group name in your Kubernetes cluster https://github.com/FairwindsOps/rbac-lookup rbac-view - Visualize Kubernetes RBAC rules https://github.com/jasonrichardsmith/rbac-view pv-df - Show disk usage (like unix df) for persistent volumes https://artifacthub.io/packages/krew/krew-index/df-pv resource-quotas sniff - tcpdump pods https://github.com/eldadru/ksniff view-secret - decode Kubernetes secrets https://github.com/elsesiy/kubectl-view-secret Exposes your local resources to kubernetes https://github.com/omrikiei/ktunnel kubectl git blame https://github.com/knight42/kubectl-blame  Upgrade  Detect deprecated resources https://github.com/FairwindsOps/pluto Fix helm chart after k8s upgrade helm plugin install https://github.com/helm/helm-mapkubeapis  Baremetal  Awesome baremetal https://github.com/alexellis/awesome-baremetal EKS Anywhere https://aws.amazon.com/eks/eks-anywhere/ GKE Anthos https://cloud.google.com/anthos/clusters Popular stack options:  Kubeadm+Flannel+Linstor+MetalLB kubeadm + terraform kubeadm CNI calico (MaaS) + Terraform + RKE RKE + terraform VIP for pods https://kube-vip.io/ vmware+kismatic+ansible CNI calico vmware PKS    Security/Firewall  Set up roles in IAM, map them to K8s groups, write RBAC bindings against those groups https://github.com/kubernetes-sigs/aws-iam-authenticator#full-configuration-format https://github.com/gravitational/wormhole Calico network policy strongswan vpn istio envoyfilters OPA  Kyverno is a policy engine designed for Kubernetes. Based on the Open Policy Agent https://kyverno.io/ cases  Sync secrets https://kyverno.io/policies/other/sync_secrets/?policytypes=Secret Disallow Secrets from Env     Vectors attack - https://github.com/cyberark/kubesploit Kubesploit https://github.com/cyberark/kubesploit/blob/assets/mitre_pic_full.png Intentionally vulnerable cluster environment to learn and practice Kubernetes security https://github.com/madhuakula/kubernetes-goat kubectl-dig - Deep Kubernetes visibility from the kubectl https://github.com/sysdiglabs/kubectl-dig Realoader configmaps and secrets https://github.com/stakater/Reloader  https://dev.to/joshduffney/kubernetes-using-configmap-subpaths-to-mount-files-3a1i    Registry  Cache images between nodes https://github.com/XenitAB/spegel Container Registry and Image Management for Kubernetes Clusters https://github.com/ContainerSolutions/trow Sync registries https://github.com/plexsystems/sinker  Chaos testing  https://chaos-mesh.org/ https://github.com/berkay-dincer/kubethanos  K8s at home  https://github.com/eddiezane/kubecon-eu-2021-automating-your-home-with-k3s-and-home-assistant-notes/blob/main/README.md https://github.com/k8s-at-home https://github.com/eddiezane/pikube https://github.com/k8s-at-home/template-cluster-k3s https://www.reddit.com/r/homelab/  ","permalink":"https://okulbida.com/posts/kubernetes_tools/","summary":"This list is being updated on regular basis\nGeneral info  https://kubernetesreadme.com/  Comparison  API comparison https://kube-api.ninja/ https://learnk8s.io/research https://docs.google.com/spreadsheets/d/1RPpyDOLFmcgxMCpABDzrsBYWpPYCIBuvAoUQLwOGoQw/edit#gid=907731238  Hosting  kapsule https://www.scaleway.com/en/ free https://cloud.okteto.com/  Secrets  External secrets management integration with k8s https://github.com/godaddy/kubernetes-external-secrets Integrate Kubernetes with 1Password https://github.com/1Password/onepassword-operator Kubernetes mutating webhook for secrets-init injection https://github.com/doitintl/kube-secrets-init AWS EKS Secrets store CSI driver https://aws.amazon.com/blogs/security/how-to-use-aws-secrets-configuration-provider-with-kubernetes-secrets-store-csi-driver/  RBAC  RBAC practices and tooling https://rbac.dev/ Visualize RBAC https://github.com/team-soteria/rback RBAC Manager is designed to simplify authorization in Kubernetes https://github.","title":"Kubernetes tools"},{"content":"  Keep the cloud provider platform secure\n Least privilege Secure traffic into cluster    Run security test in development environments\n Mirror environments    Cluster authentication \u0026amp; authorization\n Leverage OIDC for k8s authentication RBAC - define roles    In cluster network/security/micro segmentation\n Prevent namespace-to-namespace communication Network policy    Policy \u0026amp; Governance\n  k8s admission controllers\n Read only Non-privileges ports    Runtime security \u0026amp; monitoring\n Identity security privilege not needed Track anomalies Falco \u0026amp; Twistlock    Application secret management\n Encrypt Vault Sealed secrets SOPS    Data protection \u0026amp; CVE remediation\n Approved API versions Review release notes Restric ingress/egress Allow approved docker images    Auditing\n Define policies Trigger alerts Store audit logs    Container security\n Vulnerability scanning in pipeline MTLS - adhere to k8s pod security standards    ","permalink":"https://okulbida.com/posts/k8s-security-best-practices/","summary":"Keep the cloud provider platform secure\n Least privilege Secure traffic into cluster    Run security test in development environments\n Mirror environments    Cluster authentication \u0026amp; authorization\n Leverage OIDC for k8s authentication RBAC - define roles    In cluster network/security/micro segmentation\n Prevent namespace-to-namespace communication Network policy    Policy \u0026amp; Governance\n  k8s admission controllers\n Read only Non-privileges ports    Runtime security \u0026amp; monitoring","title":"k8s security best practices"},{"content":"Reasons to upgrade to k8s 1.30 Container resource based autoscaling Container resource based autoscaling is now promoted to stable https://github.com/kubernetes/enhancements/issues/1610 Horizontal Pod Autoscaler examines the total resource usage of the entire pod i.e. sum of all containers and scale pods based on average CPU or memory usage. Container resource based autoscaling feature allows HPA to scale workloads based on the resource usage of individual containers within a pod, instead of the aggregated usage of all containers in the pod\nAggregated Discovery If you use Helm \u0026amp; Flux, you should consider upgrading to Kubernetes 1.30 which now offers GA Aggregated Discovery. This is particularly useful on clusters with many CRDs, where the number of API calls Helm SDK will decrease substantially.\n","permalink":"https://okulbida.com/posts/k8s-130-version/","summary":"Reasons to upgrade to k8s 1.30 Container resource based autoscaling Container resource based autoscaling is now promoted to stable https://github.com/kubernetes/enhancements/issues/1610 Horizontal Pod Autoscaler examines the total resource usage of the entire pod i.e. sum of all containers and scale pods based on average CPU or memory usage. Container resource based autoscaling feature allows HPA to scale workloads based on the resource usage of individual containers within a pod, instead of the aggregated usage of all containers in the pod","title":"k8s 1.30 version"},{"content":"Kubernetes InPlacePodVerticalScaling feature\nKubernetes v1.27 introduces InPlacePodVerticalScaling, allowing seamless pod resource resizing without restarts\nEnhanced Continuity: Eliminates the downtime and potential data loss caused by pod restart\nCost Savings: Avoid overprovisioning and optimizing resource usage. InPlacePodVerticalScaling lets you allocate resources precisely as needed\nIn this example for pod memory resources configuration, the resizePolicy indicates that changes to the memory allocation require a restart of the container, and for CPU resources the restart is not necessary during resizing.\ncat \u0026lt;\u0026lt;EOF | kubectl apply -f - --- apiVersion: v1 kind: Pod metadata: labels: run: nginx name: nginx spec: containers: - image: nginx name: nginx resizePolicy: - resourceName: \u0026#34;memory\u0026#34; restartPolicy: \u0026#34;RestartContainer\u0026#34; - resourceName: \u0026#34;cpu\u0026#34; restartPolicy: \u0026#34;NotRequired\u0026#34; resources: limits: cpu: \u0026#34;300m\u0026#34; memory: \u0026#34;1Gi\u0026#34; requests: cpu: \u0026#34;100m\u0026#34; memory: \u0026#34;500Mi\u0026#34; EOF ","permalink":"https://okulbida.com/posts/k8s-new-features/","summary":"Kubernetes InPlacePodVerticalScaling feature\nKubernetes v1.27 introduces InPlacePodVerticalScaling, allowing seamless pod resource resizing without restarts\nEnhanced Continuity: Eliminates the downtime and potential data loss caused by pod restart\nCost Savings: Avoid overprovisioning and optimizing resource usage. InPlacePodVerticalScaling lets you allocate resources precisely as needed\nIn this example for pod memory resources configuration, the resizePolicy indicates that changes to the memory allocation require a restart of the container, and for CPU resources the restart is not necessary during resizing.","title":"k8s InPlacePodVerticalScaling"},{"content":"Collaboration  Use remote state and state locking  For certain backends like AWS S3, enable versioning to make it easier to recover your state if needed   Agree on naming convention Use meaningful tags to easily identify resources: environment, owner, project keys are must  You can also add cloud-custdodian for components which are out of terrarfom/IaC tools, which could automatically tag your manually created resources with Owner Creator based on CloudTrail events    Don\u0026rsquo;t reinvent the wheel Use existing shared and community modules. As a common sense, it\u0026rsquo;s highly recommended to reuse matured modules such as VPC. Look for these modules in Terraform Registry\nExplicit definition  Keep your providers, modules versioned properly Keep each module in a separate repo. Usually it depends on project size, and we can use monorepo or single modules repo as well.  Avoid variables hard-coding Check if you can get the value of an attribute via a data source instead of setting it explicitly. For example, instead of finding our AWS account id from the console and setting it in terraform.tfvars as\naws_account_id=”99999999999” we can get it from a data source\ndata \u0026quot;aws_caller_identity\u0026quot; \u0026quot;current\u0026quot; {} locals { account_id = data.aws_caller_identity.current.account_id } Automate   Use pre-commit https://pre-commit.com/#install https://github.com/antonbabenko/pre-commit-terraform\n  Must have hooks:\n terraform_fmt terraform_validate terraform_docs terraform_tflint checkov    DRY (Don\u0026rsquo;t repeat yourself)\n Consider using terragrunt if you need advanced dependency management. It\u0026rsquo;s also suitable if you need advanced dependency management and want to simplify remote state management    CICD\n For PRs collaboration use Atlantis For infrastructure drifts detection use https://github.com/snyk/driftctl    ","permalink":"https://okulbida.com/posts/terraform-best-practices/","summary":"Collaboration  Use remote state and state locking  For certain backends like AWS S3, enable versioning to make it easier to recover your state if needed   Agree on naming convention Use meaningful tags to easily identify resources: environment, owner, project keys are must  You can also add cloud-custdodian for components which are out of terrarfom/IaC tools, which could automatically tag your manually created resources with Owner Creator based on CloudTrail events    Don\u0026rsquo;t reinvent the wheel Use existing shared and community modules.","title":"Terraform best practices"},{"content":"Userdata is compatible with the standard AWS EKS Terraform module, with the sole recommendation being the utilization of a custom AMI. In order to use instance-store you also need to install local-static-provisioner - https://github.com/kubernetes-sigs/sig-storage-local-static-provisioner\nTerraform example:\neks-dev-instance-store = { instance_types = [\u0026#34;r6id.large\u0026#34;] min_size = 1 max_size = 3 desired_size = 1 block_device_mappings = {# Root volume  xvda = { device_name = \u0026#34;/dev/xvda\u0026#34; ebs = { volume_size = 24 volume_type = \u0026#34;gp3\u0026#34; iops = 3000 encrypted = false delete_on_termination = true } } } ami_id = data.aws_ami.ubuntu.image_id# The virtual device name (ephemeralN). Instance store volumes are numbered # starting from 0. An instance type with 2 available instance store volumes # can specify mappings for ephemeral0 and ephemeral1. The number of available # instance store volumes depends on the instance type. After you connect to # the instance, you must mount the volume - here, we are using user data to automatically # mount the volume(s) during instance creation. # # NVMe instance store volumes are automatically enumerated and assigned a device # name. Including them in your block device mapping has no effect. # post_bootstrap_user_data  enable_bootstrap_user_data = true# NVMe instance store volumes are automatically enumerated and assigned a device  pre_bootstrap_user_data = \u0026lt;\u0026lt;-EOT echo \u0026#34;Running a custom user data script\u0026#34; set -ex apt-get update apt-get install -y nvme-cli mdadm xfsprogs# Fetch the list of NVMe devices  DEVICES=$(lsblk -d -o NAME | grep nvme) DISK_ARRAY=() for DEV in $DEVICES do# Exclude the root disk, /dev/nvme0n1, from the list of devices  if [[ $${DEV} != \u0026#34;nvme0n1\u0026#34; ]]; then NVME_INFO=$(nvme id-ctrl --raw-binary \u0026#34;/dev/$${DEV}\u0026#34; | cut -c3073-3104 | tr -s \u0026#39; \u0026#39; | sed \u0026#39;s/ $//g\u0026#39;)# Check if the device is Amazon EC2 NVMe Instance Storage  if [[ $${NVME_INFO} == *\u0026#34;ephemeral\u0026#34;* ]]; then DISK_ARRAY+=(\u0026#34;/dev/$${DEV}\u0026#34;) fi fi done DISK_COUNT=$${#DISK_ARRAY[@]} if [ $${DISK_COUNT} -eq 0 ]; then echo \u0026#34;No NVMe SSD disks available. No further action needed.\u0026#34; else if [ $${DISK_COUNT} -eq 1 ]; then TARGET_DEV=$${DISK_ARRAY[0]} mkfs.xfs $${TARGET_DEV} else mdadm --create --verbose /dev/md0 --level=0 --raid-devices=$${DISK_COUNT} $${DISK_ARRAY[@]} mkfs.xfs /dev/md0 TARGET_DEV=/dev/md0 fi mkdir -p /local1 echo $${TARGET_DEV} /local1 xfs defaults,noatime 1 2 \u0026gt;\u0026gt; /etc/fstab mount -a /usr/bin/chown -hR +999:+1000 /local1 fi EOT labels = { group = \u0026#34;instance-store\u0026#34; } taints = { dedicated = { key = \u0026#34;group\u0026#34; value = \u0026#34;instance-store\u0026#34; effect = \u0026#34;NO_SCHEDULE\u0026#34; } } update_config = { max_unavailable_percentage = 25 } tags = { ExtraTag = \u0026#34;instance-store\u0026#34; \u0026#34;k8s.io/cluster-autoscaler/enabled\u0026#34; = \u0026#34;true\u0026#34; \u0026#34;k8s.io/cluster-autoscaler/${local.name}\u0026#34; = \u0026#34;owned\u0026#34; } } ","permalink":"https://okulbida.com/posts/eks-instance-store/","summary":"Userdata is compatible with the standard AWS EKS Terraform module, with the sole recommendation being the utilization of a custom AMI. In order to use instance-store you also need to install local-static-provisioner - https://github.com/kubernetes-sigs/sig-storage-local-static-provisioner\nTerraform example:\neks-dev-instance-store = { instance_types = [\u0026#34;r6id.large\u0026#34;] min_size = 1 max_size = 3 desired_size = 1 block_device_mappings = {# Root volume  xvda = { device_name = \u0026#34;/dev/xvda\u0026#34; ebs = { volume_size = 24 volume_type = \u0026#34;gp3\u0026#34; iops = 3000 encrypted = false delete_on_termination = true } } } ami_id = data.","title":"EKS with instance-store nitro-based node-group"},{"content":"There are numerous solutions for accessing private RDS instances, many of which require thoughtful design. The solution I use sometimes is straightforward: I deploy it as a Helm chart within a k8s cluster. In this setup, access to the RDS is contingent on having access to the k8s cluster with the appropriate RBAC configurations. While it may not be perfect, it\u0026rsquo;s secure, quick to implement, and requires minimal maintenance. The following command demonstrates the basic principle:\nsocat TCP4-LISTEN:8888,fork TCP4:xxxxxxxx.us-east-1.rds.amazonaws.com:5432 We execute `socat`` command on an instance or pod. Subsequently, we need to forward the port to our local machine\n","permalink":"https://okulbida.com/posts/simple-rds-access/","summary":"There are numerous solutions for accessing private RDS instances, many of which require thoughtful design. The solution I use sometimes is straightforward: I deploy it as a Helm chart within a k8s cluster. In this setup, access to the RDS is contingent on having access to the k8s cluster with the appropriate RBAC configurations. While it may not be perfect, it\u0026rsquo;s secure, quick to implement, and requires minimal maintenance. The following command demonstrates the basic principle:","title":"Simple rds access"},{"content":"After upgrading Kubernetes (k8s), you might encounter errors such as no matches for kind \u0026quot;Deployment\u0026quot; in version \u0026quot;apps/v1beta1\u0026quot;. These errors typically indicate that certain resources have become deprecated. To resolve these issues without the need to delete your Helm chart, you can follow this simple solution:\nhelm plugin install https://github.com/helm/helm-mapkubeapis helm mapkubeapis \u0026lt;releasename\u0026gt; helm upgrade \u0026lt;releasename\u0026gt; It\u0026rsquo;s important to note that you may still need to update your Helm chart templates, especially if there have been structural changes between versions. For instance, you might observe differences in the Horizontal Pod Autoscaler (HPA) resource between versions 1.24 and 1.27.\n","permalink":"https://okulbida.com/posts/helm-fix-after-k8s-upgrade/","summary":"After upgrading Kubernetes (k8s), you might encounter errors such as no matches for kind \u0026quot;Deployment\u0026quot; in version \u0026quot;apps/v1beta1\u0026quot;. These errors typically indicate that certain resources have become deprecated. To resolve these issues without the need to delete your Helm chart, you can follow this simple solution:\nhelm plugin install https://github.com/helm/helm-mapkubeapis helm mapkubeapis \u0026lt;releasename\u0026gt; helm upgrade \u0026lt;releasename\u0026gt; It\u0026rsquo;s important to note that you may still need to update your Helm chart templates, especially if there have been structural changes between versions.","title":"Resolving Helm issues after kubernetes upgrade"},{"content":"Expose Amazon EKS pods through cross-account load balancer\n https://aws.amazon.com/blogs/containers/expose-amazon-eks-pods-through-cross-account-load-balancer/  ","permalink":"https://okulbida.com/posts/eks-expose-pods-through-cross-account-lb/","summary":"Expose Amazon EKS pods through cross-account load balancer\n https://aws.amazon.com/blogs/containers/expose-amazon-eks-pods-through-cross-account-load-balancer/  ","title":"EKS expose pods through cross-account Load balancer"},{"content":"Simplified EKS access\n https://aws.amazon.com/about-aws/whats-new/2023/12/amazon-eks-controls-iam-cluster-access-management/ https://aws.amazon.com/blogs/containers/a-deep-dive-into-simplified-amazon-eks-access-management-controls/ https://github.com/hashicorp/terraform-provider-aws/issues/34982  ","permalink":"https://okulbida.com/posts/eks-access/","summary":"Simplified EKS access\n https://aws.amazon.com/about-aws/whats-new/2023/12/amazon-eks-controls-iam-cluster-access-management/ https://aws.amazon.com/blogs/containers/a-deep-dive-into-simplified-amazon-eks-access-management-controls/ https://github.com/hashicorp/terraform-provider-aws/issues/34982  ","title":"EKS simplified access"},{"content":"While using Loki with S3 and Dynamodb it\u0026rsquo;s mandatory to add provision_config details as default might affect your budget https://grafana.com/docs/loki/latest/configuration/#provision_config\n[provisioned_write_throughput: \u0026lt;int\u0026gt; | default = 3000] # DynamoDB table default read throughput. # CLI flag: -\u0026lt;prefix\u0026gt;.read-throughput [provisioned_read_throughput: \u0026lt;int\u0026gt; | default = 300] ","permalink":"https://okulbida.com/posts/loki-s3-dynamodb/","summary":"While using Loki with S3 and Dynamodb it\u0026rsquo;s mandatory to add provision_config details as default might affect your budget https://grafana.com/docs/loki/latest/configuration/#provision_config\n[provisioned_write_throughput: \u0026lt;int\u0026gt; | default = 3000] # DynamoDB table default read throughput. # CLI flag: -\u0026lt;prefix\u0026gt;.read-throughput [provisioned_read_throughput: \u0026lt;int\u0026gt; | default = 300] ","title":"Loki S3 dynamodb"},{"content":"","permalink":"https://okulbida.com/posts/bigdata-comparison-cloudproviders/","summary":"","title":"Bigdata comparison within AWS,Azure,GCP"},{"content":"","permalink":"https://okulbida.com/posts/db-cloudproviders/","summary":"","title":"Database comparison within AWS,Azure,GCP"},{"content":"Recent elasticsearch licensing change ensures that the Beats modules are sending data to an officially supported versions of Elasticsearch and Kibana where Elastic can attest to the quality and scale of the products. Does AWS have any plans to fork a version filebeat?\nhttps://www.elastic.co/guide/en/beats/libbeat/current/breaking-changes-7.13.html\nhttps://www.reddit.com/r/aws/comments/nn95aq/elastic_has_broken_filebeat_as_of_713_it_no/\nWhat are the alternatives?\n Host elasticsearch on EC2 instances, why not? CloudWatch, slow\u0026hellip; https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/CWL_ES_Stream.html Kubernetes options like ECK or helm chart deployments\u0026hellip;tricky for production usage Loki? Is it mature enough?  UPDATE 2021-07-12:\nOpenSearch 1.0 launches with Apache License, Version 2.0 and AWS huge contribution efforts\nhttps://aws.amazon.com/blogs/opensource/opensearch-1-0-launches/\n","permalink":"https://okulbida.com/posts/aws-elasticsearch-licensing/","summary":"Recent elasticsearch licensing change ensures that the Beats modules are sending data to an officially supported versions of Elasticsearch and Kibana where Elastic can attest to the quality and scale of the products. Does AWS have any plans to fork a version filebeat?\nhttps://www.elastic.co/guide/en/beats/libbeat/current/breaking-changes-7.13.html\nhttps://www.reddit.com/r/aws/comments/nn95aq/elastic_has_broken_filebeat_as_of_713_it_no/\nWhat are the alternatives?\n Host elasticsearch on EC2 instances, why not? CloudWatch, slow\u0026hellip; https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/CWL_ES_Stream.html Kubernetes options like ECK or helm chart deployments\u0026hellip;tricky for production usage Loki?","title":"Aws vs Elasticsearch licensing"},{"content":"You can now launch NAT Gateways in your VPC without associating an internet gateway to your VPC. Internet Gateway is required to provide internet access to the NAT Gateway. However, some customers use their NAT Gateways with Transit Gateway or virtual private gateway to communicate privately with other VPCs or on-premises environments and thus, do not need an internet gateway attached to their VPCs.\nMore details: https://aws.amazon.com/about-aws/whats-new/2021/06/aws-removes-nat-gateways-dependence-on-internet-gateway-for-private-communications/\n","permalink":"https://okulbida.com/posts/aws-nat-gateway-no-need-igw/","summary":"You can now launch NAT Gateways in your VPC without associating an internet gateway to your VPC. Internet Gateway is required to provide internet access to the NAT Gateway. However, some customers use their NAT Gateways with Transit Gateway or virtual private gateway to communicate privately with other VPCs or on-premises environments and thus, do not need an internet gateway attached to their VPCs.\nMore details: https://aws.amazon.com/about-aws/whats-new/2021/06/aws-removes-nat-gateways-dependence-on-internet-gateway-for-private-communications/","title":"AWS removes NAT Gateway’s dependence on Internet Gateway for Private communications"},{"content":"What’s new in Grafana v8.0\n  Grafana includes built-in support for Prometheus Alertmanager. Once you add it as a data source, you can use the Grafana alerting UI to manage silences, contact points as well as notification policies. A drop down option in these pages allows you to switch between Grafana and any configured Alertmanager data sources. https://grafana.com/docs/grafana/latest/datasources/alertmanager/\n  Prometheus metrics browser https://grafana.com/docs/grafana/latest/datasources/prometheus/#metrics-browser\n  More details: https://grafana.com/docs/grafana/latest/whatsnew/whats-new-in-v8-0/\n","permalink":"https://okulbida.com/posts/grafana-8-released/","summary":"What’s new in Grafana v8.0\n  Grafana includes built-in support for Prometheus Alertmanager. Once you add it as a data source, you can use the Grafana alerting UI to manage silences, contact points as well as notification policies. A drop down option in these pages allows you to switch between Grafana and any configured Alertmanager data sources. https://grafana.com/docs/grafana/latest/datasources/alertmanager/\n  Prometheus metrics browser https://grafana.com/docs/grafana/latest/datasources/prometheus/#metrics-browser\n  More details: https://grafana.com/docs/grafana/latest/whatsnew/whats-new-in-v8-0/","title":"What’s new in Grafana v8.0"},{"content":"https://aws.amazon.com/about-aws/whats-new/2021/05/aws-load-balancer-controller-version-2-2-available-support-nlb-instance/\n","permalink":"https://okulbida.com/posts/aws-lb-controller-nlb-support/","summary":"https://aws.amazon.com/about-aws/whats-new/2021/05/aws-load-balancer-controller-version-2-2-available-support-nlb-instance/","title":"AWS Load Balancer Controller version 2.2 now available with support for NLB instance targeting"},{"content":"Amazon EC2 Auto Scaling now natively supports Predictive Scaling so you can proactively scale out your Auto Scaling group to be ready for upcoming demand. Predictive Scaling can help you avoid the need to over-provision capacity, resulting in lower EC2 cost, while ensuring your application’s responsiveness. (Previously, Predictive Scaling was only available via AWS Auto Scaling Plans.)\nhttps://aws.amazon.com/about-aws/whats-new/2021/05/amazon-ec2-auto-scaling-introduces-predictive-scaling-native-scaling-policy/\n","permalink":"https://okulbida.com/posts/aws-ec2-predictive-autoscaling/","summary":"Amazon EC2 Auto Scaling now natively supports Predictive Scaling so you can proactively scale out your Auto Scaling group to be ready for upcoming demand. Predictive Scaling can help you avoid the need to over-provision capacity, resulting in lower EC2 cost, while ensuring your application’s responsiveness. (Previously, Predictive Scaling was only available via AWS Auto Scaling Plans.)\nhttps://aws.amazon.com/about-aws/whats-new/2021/05/amazon-ec2-auto-scaling-introduces-predictive-scaling-native-scaling-policy/","title":"Amazon EC2 Auto Scaling Introduces Predictive Scaling as a Native Scaling Policy"},{"content":"Amazon Elastic Kubernetes Service (Amazon EKS) now supports using the Amazon EKS console, CLI, and API to install and manage CoreDNS and kube-proxy in addition to existing support for the Amazon VPC CNI networking plugin.\nhttps://aws.amazon.com/about-aws/whats-new/2021/05/eks-add-ons-now-support-coredns-kube-proxy/\nhttps://docs.aws.amazon.com/eks/latest/userguide/eks-add-ons.html\n","permalink":"https://okulbida.com/posts/awsekscoredns/","summary":"Amazon Elastic Kubernetes Service (Amazon EKS) now supports using the Amazon EKS console, CLI, and API to install and manage CoreDNS and kube-proxy in addition to existing support for the Amazon VPC CNI networking plugin.\nhttps://aws.amazon.com/about-aws/whats-new/2021/05/eks-add-ons-now-support-coredns-kube-proxy/\nhttps://docs.aws.amazon.com/eks/latest/userguide/eks-add-ons.html","title":"EKS Add-Ons Now Supports CoreDNS and kube-proxy"},{"content":"Full list of videos from KubeCon 2021 Europe\nhttps://www.youtube.com/playlist?list=PLj6h78yzYM2MqBm19mRz9SYLsw4kfQBrC\n","permalink":"https://okulbida.com/posts/kubecon2021/","summary":"Full list of videos from KubeCon 2021 Europe\nhttps://www.youtube.com/playlist?list=PLj6h78yzYM2MqBm19mRz9SYLsw4kfQBrC","title":"Kubecon2021"},{"content":"AWS CloudFront functions is a nice alternative to Lambda@Edge\nhttps://aws.amazon.com/blogs/aws/introducing-cloudfront-functions-run-your-code-at-the-edge-with-low-latency-at-any-scale/\n","permalink":"https://okulbida.com/posts/aws-cloudfront-functions/","summary":"AWS CloudFront functions is a nice alternative to Lambda@Edge\nhttps://aws.amazon.com/blogs/aws/introducing-cloudfront-functions-run-your-code-at-the-edge-with-low-latency-at-any-scale/","title":"AWS CloudFront functions"},{"content":"Amazon EC2 enables you to replace the root EBS volume for a running instance\nLimitations:  You can\u0026rsquo;t replace the root volume if it is an instance store volume. You can\u0026rsquo;t replace the root volume for metal instances.  More details: https://aws.amazon.com/about-aws/whats-new/2021/04/ec2-enables-replacing-root-volumes-for-quick-restoration-and-troubleshooting/\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-restoring-volume.html#replace-root\n","permalink":"https://okulbida.com/posts/ec2-root-volume-replacing/","summary":"Amazon EC2 enables you to replace the root EBS volume for a running instance\nLimitations:  You can\u0026rsquo;t replace the root volume if it is an instance store volume. You can\u0026rsquo;t replace the root volume for metal instances.  More details: https://aws.amazon.com/about-aws/whats-new/2021/04/ec2-enables-replacing-root-volumes-for-quick-restoration-and-troubleshooting/\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-restoring-volume.html#replace-root","title":"EC2 root volume replacing"},{"content":"https://y0zg.github.io/public-blog/pf-infographic-promql-cheatsheet.pdf\n","permalink":"https://okulbida.com/posts/prom-cheat-sheet/","summary":"https://y0zg.github.io/public-blog/pf-infographic-promql-cheatsheet.pdf","title":"PromQL cheat sheet"},{"content":"How to use AWS Secrets \u0026amp; Configuration Provider with your Kubernetes Secrets Store CSI driver. One more example of kubernetes secrets management among vault, external-secrets and 1password operator 😅\nhttps://aws.amazon.com/blogs/security/how-to-use-aws-secrets-configuration-provider-with-kubernetes-secrets-store-csi-driver/\n","permalink":"https://okulbida.com/posts/aws-secrets-csi/","summary":"How to use AWS Secrets \u0026amp; Configuration Provider with your Kubernetes Secrets Store CSI driver. One more example of kubernetes secrets management among vault, external-secrets and 1password operator 😅\nhttps://aws.amazon.com/blogs/security/how-to-use-aws-secrets-configuration-provider-with-kubernetes-secrets-store-csi-driver/","title":"AWS Secrets CSI for EKS"},{"content":"Grafana was relicensed to AGPLv3\nhttps://grafana.com/blog/2021/04/20/grafana-loki-tempo-relicensing-to-agplv3/\n","permalink":"https://okulbida.com/posts/grafana-license/","summary":"Grafana was relicensed to AGPLv3\nhttps://grafana.com/blog/2021/04/20/grafana-loki-tempo-relicensing-to-agplv3/","title":"Grafana License"},{"content":"","permalink":"https://okulbida.com/categories/","summary":"","title":""},{"content":"Oleksandr Kulbida is a seasoned cloud engineer with focus on opensource. His passions are around breaking silos between teams and automating everything! Oleksandr follows Unix philosophy and loves adopting infrastructure as code. Feel free to connect on LinkedIn https://www.linkedin.com/in/oleksandrkulbida/ ","permalink":"https://okulbida.com/about/","summary":"about","title":"About"},{"content":"","permalink":"https://okulbida.com/archive/","summary":"archive","title":"Archive"}]