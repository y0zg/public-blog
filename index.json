[{"content":"This list is being updated on regular basis\nGeneral info  https://kubernetesreadme.com/  Comparison  API comparison https://kube-api.ninja/ https://learnk8s.io/research https://docs.google.com/spreadsheets/d/1RPpyDOLFmcgxMCpABDzrsBYWpPYCIBuvAoUQLwOGoQw/edit#gid=907731238  Hosting  kapsule https://www.scaleway.com/en/ free https://cloud.okteto.com/  Secrets  External secrets management integration with k8s https://github.com/godaddy/kubernetes-external-secrets Integrate Kubernetes with 1Password https://github.com/1Password/onepassword-operator Kubernetes mutating webhook for secrets-init injection https://github.com/doitintl/kube-secrets-init AWS EKS Secrets store CSI driver https://aws.amazon.com/blogs/security/how-to-use-aws-secrets-configuration-provider-with-kubernetes-secrets-store-csi-driver/  RBAC  RBAC practices and tooling https://rbac.dev/ Visualize RBAC https://github.com/team-soteria/rback RBAC Manager is designed to simplify authorization in Kubernetes https://github.com/FairwindsOps/rbac-manager Access matrix https://github.com/corneliusweig/rakkess  Security  Kubernetes vector attack https://github.com/cyberark/kubesploit  https://github.com/cyberark/kubesploit/blob/assets/mitre_pic_full.png    Production checklist  https://learnk8s.io/production-best-practices/  Deployment Helm  https://v3.helm.sh/docs/howto/charts_tips_and_tricks  Serverless  Knative https://knative.dev/docs/eventing/sources/ Kubeless https://kubeless.io/ OpenFAAS https://github.com/openfaas/faas  Local  Minikube https://minikube.sigs.k8s.io/ https://docs.tilt.dev/ k0s https://github.com/k0sproject/k0s Microk8s from Canonical https://microk8s.io/ KIND https://kind.sigs.k8s.io/ Telepresence https://www.telepresence.io/tutorials/kubernetes-rapid Exposes your local resources to kubernetes (like telepresence) https://github.com/omrikiei/ktunnel Skaffold https://skaffold.dev/  Capacity  Overview of the resource requests, limits, and utilization in a Kubernetes https://github.com/robscott/kube-capacity Recommendations requests/limits https://github.com/robusta-dev/krr  K8s Plugins  Plugin installer https://github.com/kubernetes-sigs/krew  Plugins list https://krew.sigs.k8s.io/plugins   Debug pods https://github.com/aylei/kubectl-debug Resources https://github.com/y0zg/kubectl-resources View webhook https://github.com/Trendyol/kubectl-view-webhook#kubectl-view-webhook access-matrix - show an access matrix for k8s server resources https://github.com/corneliusweig/rakkess rbac-lookup - Easily find roles and cluster roles attached to any user, service account, or group name in your Kubernetes cluster https://github.com/FairwindsOps/rbac-lookup rbac-view - Visualize Kubernetes RBAC rules https://github.com/jasonrichardsmith/rbac-view pv-df - Show disk usage (like unix df) for persistent volumes https://artifacthub.io/packages/krew/krew-index/df-pv resource-quotas sniff - tcpdump pods https://github.com/eldadru/ksniff view-secret - decode Kubernetes secrets https://github.com/elsesiy/kubectl-view-secret Exposes your local resources to kubernetes https://github.com/omrikiei/ktunnel kubectl git blame https://github.com/knight42/kubectl-blame  Upgrade  Detect deprecated resources https://github.com/FairwindsOps/pluto Fix helm chart after k8s upgrade helm plugin install https://github.com/helm/helm-mapkubeapis  Baremetal  Awesome baremetal https://github.com/alexellis/awesome-baremetal EKS Anywhere https://aws.amazon.com/eks/eks-anywhere/ GKE Anthos https://cloud.google.com/anthos/clusters Popular stack options:  Kubeadm+Flannel+Linstor+MetalLB kubeadm + terraform kubeadm CNI calico (MaaS) + Terraform + RKE RKE + terraform VIP for pods https://kube-vip.io/ vmware+kismatic+ansible CNI calico vmware PKS    Security/Firewall  Set up roles in IAM, map them to K8s groups, write RBAC bindings against those groups https://github.com/kubernetes-sigs/aws-iam-authenticator#full-configuration-format https://github.com/gravitational/wormhole Calico network policy strongswan vpn istio envoyfilters OPA  Kyverno is a policy engine designed for Kubernetes. Based on the Open Policy Agent https://kyverno.io/ cases  Sync secrets https://kyverno.io/policies/other/sync_secrets/?policytypes=Secret Disallow Secrets from Env     Vectors attack - https://github.com/cyberark/kubesploit Kubesploit https://github.com/cyberark/kubesploit/blob/assets/mitre_pic_full.png Intentionally vulnerable cluster environment to learn and practice Kubernetes security https://github.com/madhuakula/kubernetes-goat kubectl-dig - Deep Kubernetes visibility from the kubectl https://github.com/sysdiglabs/kubectl-dig Realoader configmaps and secrets https://github.com/stakater/Reloader  https://dev.to/joshduffney/kubernetes-using-configmap-subpaths-to-mount-files-3a1i    Registry  Cache images between nodes https://github.com/XenitAB/spegel Container Registry and Image Management for Kubernetes Clusters https://github.com/ContainerSolutions/trow Sync registries https://github.com/plexsystems/sinker  Chaos testing  https://chaos-mesh.org/ https://github.com/berkay-dincer/kubethanos  K8s at home  https://github.com/eddiezane/kubecon-eu-2021-automating-your-home-with-k3s-and-home-assistant-notes/blob/main/README.md https://github.com/k8s-at-home https://github.com/eddiezane/pikube https://github.com/k8s-at-home/template-cluster-k3s https://www.reddit.com/r/homelab/  ","permalink":"https://okulbida.com/posts/kubernetes_tools/","summary":"This list is being updated on regular basis\nGeneral info  https://kubernetesreadme.com/  Comparison  API comparison https://kube-api.ninja/ https://learnk8s.io/research https://docs.google.com/spreadsheets/d/1RPpyDOLFmcgxMCpABDzrsBYWpPYCIBuvAoUQLwOGoQw/edit#gid=907731238  Hosting  kapsule https://www.scaleway.com/en/ free https://cloud.okteto.com/  Secrets  External secrets management integration with k8s https://github.com/godaddy/kubernetes-external-secrets Integrate Kubernetes with 1Password https://github.com/1Password/onepassword-operator Kubernetes mutating webhook for secrets-init injection https://github.com/doitintl/kube-secrets-init AWS EKS Secrets store CSI driver https://aws.amazon.com/blogs/security/how-to-use-aws-secrets-configuration-provider-with-kubernetes-secrets-store-csi-driver/  RBAC  RBAC practices and tooling https://rbac.dev/ Visualize RBAC https://github.com/team-soteria/rback RBAC Manager is designed to simplify authorization in Kubernetes https://github.","title":"Kubernetes tools"},{"content":"  Keep the cloud provider platform secure\n Least privilege Secure traffic into cluster    Run security test in development environments\n Mirror environments    Cluster authentication \u0026amp; authorization\n Leverage OIDC for k8s authentication RBAC - define roles    In cluster network/security/micro segmentation\n Prevent namespace-to-namespace communication Network policy    Policy \u0026amp; Governance\n  k8s admission controllers\n Read only Non-privileges ports    Runtime security \u0026amp; monitoring\n Identity security privilege not needed Track anomalies Falco \u0026amp; Twistlock    Application secret management\n Encrypt Vault Sealed secrets SOPS    Data protection \u0026amp; CVE remediation\n Approved API versions Review release notes Restric ingress/egress Allow approved docker images    Auditing\n Define policies Trigger alerts Store audit logs    Container security\n Vulnerability scanning in pipeline MTLS - adhere to k8s pod security standards    ","permalink":"https://okulbida.com/posts/k8s-security-best-practices/","summary":"Keep the cloud provider platform secure\n Least privilege Secure traffic into cluster    Run security test in development environments\n Mirror environments    Cluster authentication \u0026amp; authorization\n Leverage OIDC for k8s authentication RBAC - define roles    In cluster network/security/micro segmentation\n Prevent namespace-to-namespace communication Network policy    Policy \u0026amp; Governance\n  k8s admission controllers\n Read only Non-privileges ports    Runtime security \u0026amp; monitoring","title":"k8s security best practices"},{"content":"You know what\u0026rsquo;s wild? AWS is almost twenty years old now. That\u0026rsquo;s both cool and kind of terrifying at the same time. I\u0026rsquo;ve been working with AWS for a while, and honestly, I still catch myself thinking about things the way they used to be, not how they actually work today.\nThe problem is that AWS changes constantly, but a lot of the foundational stuff has evolved in ways that aren\u0026rsquo;t super obvious. Plus, there\u0026rsquo;s a ton of outdated blog posts and documentation floating around that\u0026rsquo;ll lead you down the wrong path. I\u0026rsquo;ve definitely been burned by this more than once.\nSo I figured I\u0026rsquo;d write down some of the things that have changed that might trip you up. These are the gotchas that have bitten me or people I know.\nEC2 - It\u0026rsquo;s Not 2015 Anymore Remember when you had to stop an instance to change its security group? Yeah, you don\u0026rsquo;t need to do that anymore. Same with IAM roles - you can swap those out on a running instance now.\nEBS volumes? You can resize them, attach them, detach them - all while the instance is running. No more \u0026ldquo;oh crap, I need to take this down for maintenance\u0026rdquo; moments.\nAnd here\u0026rsquo;s one I just learned about recently: you can actually force stop or terminate instances without waiting around for that annoying timeout. Super useful when you know you\u0026rsquo;re never spinning something back up and you just want it gone.\nThey also added live migration between physical hosts, which means those \u0026ldquo;instance degradation\u0026rdquo; notices are way less common than they used to be. Instances are just\u0026hellip; more reliable now. Like, actually reliable. Not \u0026ldquo;AWS reliable\u0026rdquo; from 2015, but actually reliable.\nSpot instances used to be this weird bidding war thing where prices would jump around like crazy. Now the changes are way more gradual and predictable. You don\u0026rsquo;t feel like you\u0026rsquo;re day trading anymore.\nOh, and dedicated instances? You almost never need them. It\u0026rsquo;s been like a decade since they were required for HIPAA stuff. Most people I talk to still think they need them for compliance, but nope.\nOne more thing - AMI Block Public Access is now default for new accounts. They turned it on automatically back in 2023 for any accounts that hadn\u0026rsquo;t owned a public AMI in 90 days. Good move, AWS.\nS3 - The Eventually Consistent Myth This one still trips people up: S3 isn\u0026rsquo;t eventually consistent anymore. It\u0026rsquo;s read-after-write consistent. I know, I know, you learned in your AWS cert that it was eventually consistent. That was true once, but it\u0026rsquo;s not anymore.\nYou also don\u0026rsquo;t need to randomize the first part of your object keys anymore. That whole \u0026ldquo;spread your keys around to avoid hotspots\u0026rdquo; thing? Not really necessary these days.\nACLs are deprecated and off by default on new buckets. Block Public Access is enabled by default too. And new buckets are transparently encrypted at rest - you don\u0026rsquo;t even have to think about it.\nGlacier used to be its own separate service, which is wild to think about now. If you dig into your billing data you can still see traces of how it used to work before S3 absorbed it as storage classes.\nAnd those Glacier restore fees? They used to be genuinely terrifying and impossible to predict. AWS fixed that a while ago, but the horror stories stuck around. I still meet people who think Glacier restores are expensive and confusing. They\u0026rsquo;re not - and they\u0026rsquo;re not painfully slow anymore either.\nNetworking - VPCs and All That Jazz EC2-Classic is long gone, obviously. But here\u0026rsquo;s something that catches people: public IPv4 addresses aren\u0026rsquo;t free anymore. They cost the same as Elastic IPs now. That one hurt when I first found out.\nVPC peering used to be annoying, but now you\u0026rsquo;ve got way better options. Transit Gateway, VPC sharing between accounts, resource sharing, Cloud WAN - there\u0026rsquo;s a whole ecosystem of better ways to connect things.\nVPC Lattice exists now, which is basically AWS\u0026rsquo;s way of saying \u0026ldquo;here, use this and ignore all the networking gotchas.\u0026rdquo; It\u0026rsquo;s pretty neat. Tailscale works too if you want to go that route.\nCloudFront isn\u0026rsquo;t really networking, but it\u0026rsquo;s been in the networking section forever so I\u0026rsquo;ll mention it here. Updates used to take like 45 minutes, which was absolutely brutal. Now it\u0026rsquo;s closer to 5 minutes - which still feels like 45 when you\u0026rsquo;re waiting for CloudFormation to finish, but it\u0026rsquo;s progress.\nClassic Load Balancers (the \u0026ldquo;classic\u0026rdquo; means \u0026ldquo;deprecated\u0026rdquo; in AWS-speak) used to charge you for cross-AZ data transfer on top of the load balancer fees. ALBs with automatic zone balancing don\u0026rsquo;t charge extra for cross-AZ traffic anymore, just their LCU fees. Same with Classic Load Balancers, but watch out - Network Load Balancers still charge cross-AZ fees!\nNetwork Load Balancers didn\u0026rsquo;t support security groups originally, but they do now. That was a weird limitation.\nAvailability Zones used to be randomized between accounts - my us-east-1a was your us-east-1c. You can now use Resource Access Manager to get zone IDs and make sure you\u0026rsquo;re aligned across accounts. Super useful for multi-account setups.\nLambda - It\u0026rsquo;s Grown Up Lambda used to have a 5 minute timeout and no container image support. Now you can run them for 15 minutes, use Docker images, mount EFS for shared storage, give them up to 10GB of RAM (CPU scales automatically), and give /tmp up to 10GB instead of that measly 512MB.\nInvoking a Lambda in a VPC used to be dog-slow. Not anymore.\nAnd cold starts? They\u0026rsquo;re still a thing, but they\u0026rsquo;re not the massive problem they used to be. The whole \u0026ldquo;Lambda is unusable because of cold starts\u0026rdquo; argument doesn\u0026rsquo;t really hold water anymore for most use cases.\nEFS - The IOPS Problem You used to have to fill up an EFS volume with useless data to get your IOPS allocation up to something usable. Now you can adjust IOPS separately from capacity. They added a second knob, basically. Much better.\nEBS - Performance and Multi-Attach New empty EBS volumes get full performance immediately. But if you create a volume from a snapshot, you\u0026rsquo;ll want to read the entire disk with dd or similar because it lazy-loads from S3. The first read of each block will be slow. If you\u0026rsquo;re in a hurry, there are more expensive options, but reading the whole thing usually works fine.\nOh, and EBS volumes can be attached to multiple EC2 instances at the same time now (if you\u0026rsquo;re using io1). But honestly, you probably don\u0026rsquo;t want to do this. It\u0026rsquo;s one of those \u0026ldquo;just because you can doesn\u0026rsquo;t mean you should\u0026rdquo; situations.\nDynamoDB - Empty Fields and Pricing You can have empty fields in DynamoDB items now. I know someone whose system still uses a field called empty because it predates this change. That\u0026rsquo;s how long this has been a thing.\nPerformance has gotten way more reliable. You don\u0026rsquo;t need those support-only tools locked behind NDAs to see your hot key problems anymore - there are better ways to diagnose issues.\nWith the pricing changes, you almost certainly want to run everything On Demand unless you\u0026rsquo;re in a very specific situation. The math just works out differently now.\nCost Stuff - Reserved Instances Are Dying Reserved Instances are slowly going away. Savings Plans are the future. The savings rates have diverged though - they don\u0026rsquo;t offer as deep discounts as RIs used to, but they\u0026rsquo;re way more flexible. Pay attention to this, because the economics have changed.\nEC2 charges by the second now, so spinning up instances for a few minutes doesn\u0026rsquo;t cost you a full hour anymore. That\u0026rsquo;s saved me a bunch of money on test workloads.\nThe Cost Anomaly Detector has gotten really good at flagging sudden spend changes. And it\u0026rsquo;s free! Use it.\nCompute Optimizer now does EBS volumes and other things too. Its recommendations are actually trustworthy, unlike Trusted Advisor\u0026rsquo;s various\u0026hellip; suggestions. Trusted Advisor is still kind of sketchy and self-contradictory, though some of their cost checks can route through Compute Optimizer now, which helps.\nAuthentication - IAM Users Are Legacy IAM roles are where permissions should live. IAM users are for legacy applications, not humans. IAM Identity Center (the replacement for \u0026ldquo;AWS SSO\u0026rdquo;) is how humans should access AWS accounts. This causes friction sometimes, but it\u0026rsquo;s the right way to do things.\nYou can have multiple MFA devices on the root account now. And you don\u0026rsquo;t need root credentials configured for organization member accounts anymore.\nRandom Stuff That\u0026rsquo;s Changed us-east-1 is no longer a dumpster fire. I mean, it\u0026rsquo;s still us-east-1, but it\u0026rsquo;s way more stable than it used to be. Actually, AWS in general is way more durable. Outages are noteworthy events now instead of \u0026ldquo;it\u0026rsquo;s another Tuesday afternoon.\u0026rdquo;\nDeprecations are still rare, but they\u0026rsquo;re definitely happening more often. If a service sounds niche or goofy, maybe think about your exit strategy before building on top of it.\nCloudWatch doesn\u0026rsquo;t have that weird thing where the last datapoint is super low due to data inconsistency. So if your graphs suddenly drop to zero, your app actually just broke. It\u0026rsquo;s not a CloudWatch quirk anymore.\nYou can close AWS accounts in your organization from the root account now, instead of having to log into each member account as root. Small thing, but super convenient.\nWrapping Up AWS has changed a lot over the years, and it\u0026rsquo;s easy to get stuck thinking about things the old way. I\u0026rsquo;ve definitely made mistakes because I assumed something worked the way it did five years ago. Hopefully this helps you avoid some of those same pitfalls.\nThe platform keeps evolving, and that\u0026rsquo;s mostly good. But it does mean you have to stay on top of things, or at least be aware that your assumptions might be outdated.\nWhat outdated AWS knowledge have you been holding onto? I\u0026rsquo;m sure there are more things I\u0026rsquo;m missing here.\n Inspired by Last Week in AWS. Always check the official AWS documentation for the most current information.\n","permalink":"https://okulbida.com/posts/aws-2025-things-you-think-you-know/","summary":"You know what\u0026rsquo;s wild? AWS is almost twenty years old now. That\u0026rsquo;s both cool and kind of terrifying at the same time. I\u0026rsquo;ve been working with AWS for a while, and honestly, I still catch myself thinking about things the way they used to be, not how they actually work today.\nThe problem is that AWS changes constantly, but a lot of the foundational stuff has evolved in ways that aren\u0026rsquo;t super obvious.","title":"AWS in 2025: Things You Think You Know That Are Actually Wrong"},{"content":"Another year, another AWS re:Invent has come and gone. I\u0026rsquo;ve been following the announcements closely, and there are some genuinely interesting developments worth discussing.\nThe Big Picture This year\u0026rsquo;s re:Invent felt a bit different. The pre:Invent announcements started later than usual (mid-November instead of early October), and the keynote felt more focused on GenAI than infrastructure improvements. That said, there are still plenty of practical enhancements that can make our lives easier.\nSecurity Features That Matter AWS Security Agent (Preview) This one caught my attention. AWS is introducing an AI-powered security agent that can perform automated security reviews and penetration testing. While I\u0026rsquo;m always a bit skeptical of \u0026ldquo;AI solves everything\u0026rdquo; claims, this could be useful for teams that need security validation but don\u0026rsquo;t want to go through the vendor procurement process. I\u0026rsquo;m curious to see how it performs in real-world scenarios.\nIAM \u0026amp; Access Management Improvements IAM Identity Federation for External Services with JWTs - Finally! AWS is acknowledging that we live in a multi-cloud world. This feature allows you to use IAM to access other cloud providers without managing long-lived tokens. This is particularly interesting for the upcoming European Sovereign Cloud launch.\nIAM Policy Autopilot - Generate IAM policies from your code. This could help with least-privilege implementations, though I\u0026rsquo;ll be watching closely to make sure it doesn\u0026rsquo;t hallucinate actions like vpc:AuthorizedSecurityGroupIngress (yes, that\u0026rsquo;s a real concern).\nConsole Credentials for AWS CLI/SDK - A new aws login command that uses your console session for CLI authentication. It\u0026rsquo;s better than long-lived credentials, but enterprises should still be using Identity Center for proper SSO.\nS3 Security Enhancements S3 Block Public Access Organization-Level Enforcement - This is implemented via an AWS Organizational Policy, similar to Security Hub policies. The interesting part is that unlike IAM Organization Policies, a deny doesn\u0026rsquo;t automatically trump an allow. You can set \u0026quot;@@assign\u0026quot;: \u0026quot;all\u0026quot; at the root OU and override with \u0026quot;@@assign\u0026quot;: \u0026quot;none\u0026quot; on specific accounts. It\u0026rsquo;s not perfect (you can\u0026rsquo;t fine-tune individual BPA controls), but it helps with legacy bucket management.\nS3 Attribute-Based Access Control - This was a major gap in Resource Control Policies (RCPs). Now you can write RCPs that grant or deny permissions based on tags on the bucket itself. This opens up new possibilities for governance.\nS3 Bucket-Level Encryption Standardization - Here\u0026rsquo;s something important: AWS is disabling SSE-C (server-side encryption with customer-provided keys) for all new buckets starting in April 2026. SSE-C is a legacy capability from the pre-KMS days that ransomware groups have been known to exploit. Unless you have a very specific use case, you probably don\u0026rsquo;t need it.\nThreat Detection \u0026amp; Response Security Incident Response - Now available with metered pricing and a free tier. The agentic AI-powered investigation feature is interesting, though I can\u0026rsquo;t help but wonder if this is AWS\u0026rsquo;s way of reducing human security analysts. Time will tell if it\u0026rsquo;s effective.\nGuardDuty Extended Threat Detection - Now supports EC2 and ECS. These extended detections alert as Critical (severity 9+), which helps cut through the noise that GuardDuty is known for.\nSecurity Hub 2.0 - This is a significant update. The original Security Hub is now called \u0026ldquo;Security Hub CSPM,\u0026rdquo; and there\u0026rsquo;s a new \u0026ldquo;Security Hub\u0026rdquo; that focuses on near real-time risk analytics. It\u0026rsquo;s AWS\u0026rsquo;s response to Google\u0026rsquo;s security offerings, but it still requires running AWS Config Recorders, which haven\u0026rsquo;t been upgraded to support modern organizational management. I\u0026rsquo;m reserving judgment until I can test it.\nCloudTrail Updates Two new features, though they don\u0026rsquo;t address the major pain points:\nCloudTrail Data Event Aggregation - You still need expensive data events enabled to use this. It adds a 30% cost on top of data event costs, and most use cases could be solved with an Athena query. Disappointing.\nSimplified CloudTrail Events in CloudWatch - A new method for pushing CloudTrail events into CloudWatch that doesn\u0026rsquo;t require creating a CloudTrail. The pricing model is different ($0.75/GB instead of per-event), but I\u0026rsquo;m not convinced this makes data events any less expensive.\nCloud Governance \u0026amp; Cost Management AWS Organizations Billing Delegation - This is huge for companies buying AWS through resellers. Previously, the reseller had to control the Organization\u0026rsquo;s Management Account for billing. Now customers can get all the security benefits of AWS Organizations while the reseller handles billing separately.\nCloudFront Flat Rate Pricing Plans - Single flat rate for CDN, WAF, DDoS protection, and logging. Read the fine print on what happens when you exceed your performance allocation, but this should drive adoption of basic edge security controls.\nCloudFormation StackSets Improvements - Deployment ordering and enhanced configuration drift detection. StackSets have always been tools for invariants, but they\u0026rsquo;ve struggled with complexity. These updates might help, though I\u0026rsquo;ll believe it when I see it at scale.\nTag Validation in CloudFormation, Terraform, and Pulumi - This could be a game-changer. You can now validate and enforce required tags before deployment, aborting Terraform plans before they mess up production. I\u0026rsquo;ve struggled with Tag Policies and SCPs breaking pipelines, so this might finally solve that problem.\nServerless \u0026amp; Compute Lambda Tenant Isolation Mode - Route invocations to specific execution environments using tenant identifiers. This is useful for multi-tenant applications where you need strict isolation.\nLambda Managed Instances - Run Lambda functions on your EC2 instances while maintaining Lambda\u0026rsquo;s operational simplicity. It\u0026rsquo;s like reverse Fargate. The irony of making serverless better by adding servers isn\u0026rsquo;t lost on me.\nStep Functions Local Testing - New TestState API for local testing. Step Functions have been painful to author due to unclear docs and lack of examples, so this should help.\nAPI Gateway MCP Proxy Support - Transform REST APIs into Model Context Protocol (MCP)-compatible endpoints, making them accessible to AI agents. This is part of AWS\u0026rsquo;s broader GenAI push.\nGenAI \u0026amp; Bedrock AWS continues to invest heavily in Bedrock, which seems like the right call in the GenAI space:\n Bedrock Reserved Service Tier - For predictable workloads OpenAI Responses API Support - Compatibility improvements 18 New Open Weight Models - Largest expansion to date Bedrock AgentCore Updates - Policy and Evaluations in preview AWS AI Factories - The Outposts team finding a way to meet GenAI OKRs  There\u0026rsquo;s also some confusion with MCP servers - AWS announced the AWS API MCP Server in Marketplace, then announced a deprecation and consolidation into a new AWS MCP Server in preview. It makes you wonder if AWS knows what it\u0026rsquo;s doing in the GenAI space.\nNetworking \u0026amp; Infrastructure AWS Interconnect MultiCloud (Preview) - A new service announced the night before re:Invent with minimal fanfare. MultiCloud Interconnect is in preview with GCP support now, Azure coming in 2026. Pricing is TBD, and preview connections will be removed at GA, so be warned.\nNetwork Firewall Active Threat Defense as Default - Making threat defense opt-out instead of opt-in is the right security move.\nAWS STS IPv6 Support - I was experimenting with IPv6 Egress Only Gateways and was surprised by AWS\u0026rsquo;s lack of IPv6 service support. IPv6 RFCs were written at the start of my career, and I expect I\u0026rsquo;ll retire before it\u0026rsquo;s widely supported. But hey, progress is progress.\nThe Random Stuff S3 Maximum Object Size Increased to 50 TB - I\u0026rsquo;m sure there\u0026rsquo;s someone out there who needs this. For the rest of us, if you\u0026rsquo;re creating 50TB objects, you might want to rethink your architecture.\nCloudWatch Unified Management - New unified management and analytics for operational, security, and compliance data. Most organizations I know use SIEMs or data aggregation tools, so I\u0026rsquo;m not sure what pain point this solves.\nAWS DevOps Agent (Preview) - An \u0026ldquo;agentic AI\u0026rdquo; for operational excellence. After building loyalty with DevOps professionals for 15 years, AWS is now offering to replace you with AI. I wonder how this will handle the next us-east-1 outage. \u0026ldquo;We recommend migrating this workload to OCI\u0026rdquo; indeed.\nFinal Thoughts This year\u0026rsquo;s re:Invent felt more incremental than revolutionary. The GenAI focus is understandable but sometimes feels like it\u0026rsquo;s coming at the expense of core infrastructure improvements. That said, there are some genuinely useful features here, especially around security and governance.\nThe S3 security improvements are particularly welcome, and the Organizations billing delegation is a big win for reseller customers. I\u0026rsquo;m cautiously optimistic about Security Hub 2.0, though I\u0026rsquo;ll need to see it in action before I\u0026rsquo;m convinced.\nWhat are your thoughts on this year\u0026rsquo;s announcements? Are there any features you\u0026rsquo;re particularly excited about or disappointed by? Let me know in the comments or reach out on social media.\n Note: This recap is based on publicly available AWS announcements. Some features are in preview and may change before general availability. Always check the official AWS documentation for the latest information.\n","permalink":"https://okulbida.com/posts/aws-reinvent-2025-recap/","summary":"Another year, another AWS re:Invent has come and gone. I\u0026rsquo;ve been following the announcements closely, and there are some genuinely interesting developments worth discussing.\nThe Big Picture This year\u0026rsquo;s re:Invent felt a bit different. The pre:Invent announcements started later than usual (mid-November instead of early October), and the keynote felt more focused on GenAI than infrastructure improvements. That said, there are still plenty of practical enhancements that can make our lives easier.","title":"AWS re:Invent 2025 Recap: Key Announcements for Cloud Practitioners"},{"content":"Disclaimer: here you might not find something new if you know 12 factors app.\nThe 12-factor app methodology 12factor.net, is a set of principles designed to enable applications to be built with portability and resilience when deployed to the web. These principles focus on declarative formats for automation, clean contracts with the operating system, and suitability for deployment on modern cloud platforms, thus minimizing divergence between development and production, enabling continuous deployment for maximum agility.\nHere\u0026rsquo;s how each factor is applied and the benefits they bring, drawn from firsthand experiences in the field:\n1. Codebase Use Git repositories to store your application code. Different branches can align with different deployments through CI/CD pipelines.\n2. Dependencies Explicitly declare and isolate dependencies: Managing dependencies via a declaration document (package.json) and ensuring no implicit reliance on system-wide packages ensures that the app runs consistently across all environments.\nIn k8s, containerization usage can encapsulate environments perfectly to maintain this isolation.\n3. Config Store config in the environment: Configuration variables (like database URLs and external service credentials) should be stored in the environment to keep configuration dynamic and secure. This separation of config from code helps in avoiding security risks and making the app environment agnostic, simplifying deployment across different environments.\nIn k8s, utilize ConfigMaps, Secrets, mutation webhook to store and manage environmental configurations separately from the application code. This keeps sensitive information out of your codebase. Your sensitive data should be preferrably injected into memory or mounted against added as env variable.\n4. Backing services Treat backing services as attached resources: Services like databases, queueing systems, and caching systems should be treated as attachable resources, which can be replaced or attached without code changes. This makes scaling and migrating to different services easier without major codebase changes.\n5. Build, release, run CI/CD pipelines should handle the build and deployment stages separately, ensuring that the build stage produces an immutable artifact that moves to the release and run stages.\n6. Processes Execute the app as one or more stateless processes: State should be externalized (e.g., using a database or caching layer), allowing the application to scale horizontally without side-effects. Stateless applications are easier to manage and scale, as each process can be started or stopped independently.\nThis also aligns with Unix philosophy and CNCF principle where systems must have clear separation between their processes.\nCNCF principal:\nCloud native systems have a clear separation between their processes [2]. They utilize the Unix philosophy of doing one thing and doing it well. These microservices usually use technologies such as containers and aim for one process per container [3]. As such, cloud native applications should have all of their dependencies packaged in the container during the build phase and leveraged during deployment [4]. In k8s world we don\u0026rsquo;t store pod state internally. Use external data stores like database or cache to manage state.\n7. Port binding Containers within pods expose ports which are then mapped to services that abstract these details away, allowing external traffic to reach the containers.\n8. Concurrency Use k8s HPA to manage the scaling of applications based on CPU usage or other metrics, effectively handling concurrency.\n9. Disposability If your application doesn’t gracefully shut down when receiving a SIGTERM you can use kubernetes lifecycle preStop hook to trigger a graceful shutdown. You can set terminationGracePeriodSeconds to change graceful shutdown time to ensure SIGKILL isn\u0026rsquo;t sent forcibly.\n10. Dev/prod parity Maintain similar configurations across environments using Kubernetes manifests or Helm charts values, minimizing drift between environments.\n11. Logs Implement logging at the application level, exporting logs to stdout and preferrably in JSON format. Kubernetes then aggregates these logs, which can be collected and analyzed by logs collectors like Logstash, Fluentd, etc.\n12. Admin processes Use k8s Job for one-off tasks like database migrations or batch jobs. This ensures that these tasks run in an environment identical to the regular application environment.\nWhich factors do you think are missing in 12 factors but present in k8s? ;)\nIn addition to the traditional 12 factors, there are more principles considered crucial for modern production environments, especially in cloud platforms like Kubernetes.\n13.Observability Ensures applications provide insights into their internal states and metrics, crucial for managing distributed systems effectively. Traces, metrics, profiling are very useful and loved by developers.\n14. Schedulability Focuses on predictable resource demands, enabling better scheduling and stability across distributed services. K8s requests, limits are your good friends, specially if you are on lower versions of k8s.\n15. Upgradability Pertains to easy and seamless updates and data format evolution without downtime, maintaining forward compatibility. Here we should mention such upgrades like rolling, blue-green, canary, progressive delivery and more.\n16. Least privilege Involves running processes with the minimal set of permissions necessary, enhancing security\n17. Auditability Allows tracking of who did what and when, which is essential for security and compliance\n18. Security Concerns the hardening of applications against unauthorized access and attacks. Usage of tools like OPA, Kyverno, Falso is a good start.\n19. Measurability Involves quantifying resource usage and application performance, important for cost management and scaling decisions. Tools like Anodot or kubecost are very cool for understanding your bills.\nOther useful resources:\n SRE Principles SRE lessons learned CAP theorem AWS Well-architechted framework CNCF principles  ","permalink":"https://okulbida.com/posts/12-factors/","summary":"Disclaimer: here you might not find something new if you know 12 factors app.\nThe 12-factor app methodology 12factor.net, is a set of principles designed to enable applications to be built with portability and resilience when deployed to the web. These principles focus on declarative formats for automation, clean contracts with the operating system, and suitability for deployment on modern cloud platforms, thus minimizing divergence between development and production, enabling continuous deployment for maximum agility.","title":"12 Factors vs kubernetes world"},{"content":"Recent elasticsearch licensing change ensures that the Beats modules are sending data to an officially supported versions of Elasticsearch and Kibana where Elastic can attest to the quality and scale of the products. Does AWS have any plans to fork a version filebeat?\nhttps://www.elastic.co/guide/en/beats/libbeat/current/breaking-changes-7.13.html\nhttps://www.reddit.com/r/aws/comments/nn95aq/elastic_has_broken_filebeat_as_of_713_it_no/\nWhat are the alternatives?\n Host elasticsearch on EC2 instances, why not? CloudWatch, slow\u0026hellip; https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/CWL_ES_Stream.html Kubernetes options like ECK or helm chart deployments\u0026hellip;tricky for production usage Loki? Is it mature enough?  UPDATE 2021-07-12: OpenSearch 1.0 launches with Apache License, Version 2.0 and AWS huge contribution efforts\nhttps://aws.amazon.com/blogs/opensource/opensearch-1-0-launches/\nUPDATE 2024-04-21:  Quickwit - very interesting alternative for common logging tools written on rust Elasticsearch can be deployed with helm chart but based on experience you should consider using single-AZ. This might relevant to logs which are not as critical as business data. Loki is pretty good solution, it integrates well with Grafana Tempo for tracing purposes and Opentelemetry. But based on my experience, developers for some reasons don\u0026rsquo;t love search capabilities in Grafana Loki as it\u0026rsquo;s available in kibana or other commercial logging solutions like datadog.  ","permalink":"https://okulbida.com/posts/aws-elasticsearch-licensing/","summary":"Recent elasticsearch licensing change ensures that the Beats modules are sending data to an officially supported versions of Elasticsearch and Kibana where Elastic can attest to the quality and scale of the products. Does AWS have any plans to fork a version filebeat?\nhttps://www.elastic.co/guide/en/beats/libbeat/current/breaking-changes-7.13.html\nhttps://www.reddit.com/r/aws/comments/nn95aq/elastic_has_broken_filebeat_as_of_713_it_no/\nWhat are the alternatives?\n Host elasticsearch on EC2 instances, why not? CloudWatch, slow\u0026hellip; https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/CWL_ES_Stream.html Kubernetes options like ECK or helm chart deployments\u0026hellip;tricky for production usage Loki?","title":"Aws vs Elasticsearch licensing"},{"content":"Reasons to upgrade to k8s 1.30 Container resource based autoscaling Container resource based autoscaling is now promoted to stable https://github.com/kubernetes/enhancements/issues/1610 Horizontal Pod Autoscaler examines the total resource usage of the entire pod i.e. sum of all containers and scale pods based on average CPU or memory usage. Container resource based autoscaling feature allows HPA to scale workloads based on the resource usage of individual containers within a pod, instead of the aggregated usage of all containers in the pod\nAggregated Discovery If you use Helm \u0026amp; Flux, you should consider upgrading to Kubernetes 1.30 which now offers GA Aggregated Discovery. This is particularly useful on clusters with many CRDs, where the number of API calls Helm SDK will decrease substantially.\n","permalink":"https://okulbida.com/posts/k8s-130-version/","summary":"Reasons to upgrade to k8s 1.30 Container resource based autoscaling Container resource based autoscaling is now promoted to stable https://github.com/kubernetes/enhancements/issues/1610 Horizontal Pod Autoscaler examines the total resource usage of the entire pod i.e. sum of all containers and scale pods based on average CPU or memory usage. Container resource based autoscaling feature allows HPA to scale workloads based on the resource usage of individual containers within a pod, instead of the aggregated usage of all containers in the pod","title":"k8s 1.30 version"},{"content":"Kubernetes InPlacePodVerticalScaling feature\nKubernetes v1.27 introduces InPlacePodVerticalScaling, allowing seamless pod resource resizing without restarts\nEnhanced Continuity: Eliminates the downtime and potential data loss caused by pod restart\nCost Savings: Avoid overprovisioning and optimizing resource usage. InPlacePodVerticalScaling lets you allocate resources precisely as needed\nIn this example for pod memory resources configuration, the resizePolicy indicates that changes to the memory allocation require a restart of the container, and for CPU resources the restart is not necessary during resizing.\ncat \u0026lt;\u0026lt;EOF | kubectl apply -f - --- apiVersion: v1 kind: Pod metadata: labels: run: nginx name: nginx spec: containers: - image: nginx name: nginx resizePolicy: - resourceName: \u0026#34;memory\u0026#34; restartPolicy: \u0026#34;RestartContainer\u0026#34; - resourceName: \u0026#34;cpu\u0026#34; restartPolicy: \u0026#34;NotRequired\u0026#34; resources: limits: cpu: \u0026#34;300m\u0026#34; memory: \u0026#34;1Gi\u0026#34; requests: cpu: \u0026#34;100m\u0026#34; memory: \u0026#34;500Mi\u0026#34; EOF ","permalink":"https://okulbida.com/posts/k8s-new-features/","summary":"Kubernetes InPlacePodVerticalScaling feature\nKubernetes v1.27 introduces InPlacePodVerticalScaling, allowing seamless pod resource resizing without restarts\nEnhanced Continuity: Eliminates the downtime and potential data loss caused by pod restart\nCost Savings: Avoid overprovisioning and optimizing resource usage. InPlacePodVerticalScaling lets you allocate resources precisely as needed\nIn this example for pod memory resources configuration, the resizePolicy indicates that changes to the memory allocation require a restart of the container, and for CPU resources the restart is not necessary during resizing.","title":"k8s InPlacePodVerticalScaling"},{"content":"Collaboration  Use remote state and state locking  For certain backends like AWS S3, enable versioning to make it easier to recover your state if needed   Agree on naming convention Use meaningful tags to easily identify resources: environment, owner, project keys are must  You can also add cloud-custdodian for components which are out of terrarfom/IaC tools, which could automatically tag your manually created resources with Owner Creator based on CloudTrail events    Don\u0026rsquo;t reinvent the wheel Use existing shared and community modules. As a common sense, it\u0026rsquo;s highly recommended to reuse matured modules such as VPC. Look for these modules in Terraform Registry\nExplicit definition  Keep your providers, modules versioned properly Keep each module in a separate repo. Usually it depends on project size, and we can use monorepo or single modules repo as well.  Avoid variables hard-coding Check if you can get the value of an attribute via a data source instead of setting it explicitly. For example, instead of finding our AWS account id from the console and setting it in terraform.tfvars as\naws_account_id=”99999999999” we can get it from a data source\ndata \u0026quot;aws_caller_identity\u0026quot; \u0026quot;current\u0026quot; {} locals { account_id = data.aws_caller_identity.current.account_id } Automate   Use pre-commit https://pre-commit.com/#install https://github.com/antonbabenko/pre-commit-terraform\n  Must have hooks:\n terraform_fmt terraform_validate terraform_docs terraform_tflint checkov    DRY (Don\u0026rsquo;t repeat yourself)\n Consider using terragrunt if you need advanced dependency management. It\u0026rsquo;s also suitable if you need advanced dependency management and want to simplify remote state management    CICD\n For PRs collaboration use Atlantis For infrastructure drifts detection use https://github.com/snyk/driftctl    ","permalink":"https://okulbida.com/posts/terraform-best-practices/","summary":"Collaboration  Use remote state and state locking  For certain backends like AWS S3, enable versioning to make it easier to recover your state if needed   Agree on naming convention Use meaningful tags to easily identify resources: environment, owner, project keys are must  You can also add cloud-custdodian for components which are out of terrarfom/IaC tools, which could automatically tag your manually created resources with Owner Creator based on CloudTrail events    Don\u0026rsquo;t reinvent the wheel Use existing shared and community modules.","title":"Terraform best practices"},{"content":"Userdata is compatible with the standard AWS EKS Terraform module, with the sole recommendation being the utilization of a custom AMI. In order to use instance-store you also need to install local-static-provisioner - https://github.com/kubernetes-sigs/sig-storage-local-static-provisioner\nTerraform example:\neks-dev-instance-store = { instance_types = [\u0026#34;r6id.large\u0026#34;] min_size = 1 max_size = 3 desired_size = 1 block_device_mappings = {# Root volume  xvda = { device_name = \u0026#34;/dev/xvda\u0026#34; ebs = { volume_size = 24 volume_type = \u0026#34;gp3\u0026#34; iops = 3000 encrypted = false delete_on_termination = true } } } ami_id = data.aws_ami.ubuntu.image_id# The virtual device name (ephemeralN). Instance store volumes are numbered # starting from 0. An instance type with 2 available instance store volumes # can specify mappings for ephemeral0 and ephemeral1. The number of available # instance store volumes depends on the instance type. After you connect to # the instance, you must mount the volume - here, we are using user data to automatically # mount the volume(s) during instance creation. # # NVMe instance store volumes are automatically enumerated and assigned a device # name. Including them in your block device mapping has no effect. # post_bootstrap_user_data  enable_bootstrap_user_data = true# NVMe instance store volumes are automatically enumerated and assigned a device  pre_bootstrap_user_data = \u0026lt;\u0026lt;-EOT echo \u0026#34;Running a custom user data script\u0026#34; set -ex apt-get update apt-get install -y nvme-cli mdadm xfsprogs# Fetch the list of NVMe devices  DEVICES=$(lsblk -d -o NAME | grep nvme) DISK_ARRAY=() for DEV in $DEVICES do# Exclude the root disk, /dev/nvme0n1, from the list of devices  if [[ $${DEV} != \u0026#34;nvme0n1\u0026#34; ]]; then NVME_INFO=$(nvme id-ctrl --raw-binary \u0026#34;/dev/$${DEV}\u0026#34; | cut -c3073-3104 | tr -s \u0026#39; \u0026#39; | sed \u0026#39;s/ $//g\u0026#39;)# Check if the device is Amazon EC2 NVMe Instance Storage  if [[ $${NVME_INFO} == *\u0026#34;ephemeral\u0026#34;* ]]; then DISK_ARRAY+=(\u0026#34;/dev/$${DEV}\u0026#34;) fi fi done DISK_COUNT=$${#DISK_ARRAY[@]} if [ $${DISK_COUNT} -eq 0 ]; then echo \u0026#34;No NVMe SSD disks available. No further action needed.\u0026#34; else if [ $${DISK_COUNT} -eq 1 ]; then TARGET_DEV=$${DISK_ARRAY[0]} mkfs.xfs $${TARGET_DEV} else mdadm --create --verbose /dev/md0 --level=0 --raid-devices=$${DISK_COUNT} $${DISK_ARRAY[@]} mkfs.xfs /dev/md0 TARGET_DEV=/dev/md0 fi mkdir -p /local1 echo $${TARGET_DEV} /local1 xfs defaults,noatime 1 2 \u0026gt;\u0026gt; /etc/fstab mount -a /usr/bin/chown -hR +999:+1000 /local1 fi EOT labels = { group = \u0026#34;instance-store\u0026#34; } taints = { dedicated = { key = \u0026#34;group\u0026#34; value = \u0026#34;instance-store\u0026#34; effect = \u0026#34;NO_SCHEDULE\u0026#34; } } update_config = { max_unavailable_percentage = 25 } tags = { ExtraTag = \u0026#34;instance-store\u0026#34; \u0026#34;k8s.io/cluster-autoscaler/enabled\u0026#34; = \u0026#34;true\u0026#34; \u0026#34;k8s.io/cluster-autoscaler/${local.name}\u0026#34; = \u0026#34;owned\u0026#34; } } ","permalink":"https://okulbida.com/posts/eks-instance-store/","summary":"Userdata is compatible with the standard AWS EKS Terraform module, with the sole recommendation being the utilization of a custom AMI. In order to use instance-store you also need to install local-static-provisioner - https://github.com/kubernetes-sigs/sig-storage-local-static-provisioner\nTerraform example:\neks-dev-instance-store = { instance_types = [\u0026#34;r6id.large\u0026#34;] min_size = 1 max_size = 3 desired_size = 1 block_device_mappings = {# Root volume  xvda = { device_name = \u0026#34;/dev/xvda\u0026#34; ebs = { volume_size = 24 volume_type = \u0026#34;gp3\u0026#34; iops = 3000 encrypted = false delete_on_termination = true } } } ami_id = data.","title":"EKS with instance-store nitro-based node-group"},{"content":"There are numerous solutions for accessing private RDS instances, many of which require thoughtful design. The solution I use sometimes is straightforward: I deploy it as a Helm chart within a k8s cluster. In this setup, access to the RDS is contingent on having access to the k8s cluster with the appropriate RBAC configurations. While it may not be perfect, it\u0026rsquo;s secure, quick to implement, and requires minimal maintenance. The following command demonstrates the basic principle:\nsocat TCP4-LISTEN:8888,fork TCP4:xxxxxxxx.us-east-1.rds.amazonaws.com:5432 We execute `socat`` command on an instance or pod. Subsequently, we need to forward the port to our local machine\n","permalink":"https://okulbida.com/posts/simple-rds-access/","summary":"There are numerous solutions for accessing private RDS instances, many of which require thoughtful design. The solution I use sometimes is straightforward: I deploy it as a Helm chart within a k8s cluster. In this setup, access to the RDS is contingent on having access to the k8s cluster with the appropriate RBAC configurations. While it may not be perfect, it\u0026rsquo;s secure, quick to implement, and requires minimal maintenance. The following command demonstrates the basic principle:","title":"Simple rds access"},{"content":"After upgrading Kubernetes (k8s), you might encounter errors such as no matches for kind \u0026quot;Deployment\u0026quot; in version \u0026quot;apps/v1beta1\u0026quot;. These errors typically indicate that certain resources have become deprecated. To resolve these issues without the need to delete your Helm chart, you can follow this simple solution:\nhelm plugin install https://github.com/helm/helm-mapkubeapis helm mapkubeapis \u0026lt;releasename\u0026gt; helm upgrade \u0026lt;releasename\u0026gt; It\u0026rsquo;s important to note that you may still need to update your Helm chart templates, especially if there have been structural changes between versions. For instance, you might observe differences in the Horizontal Pod Autoscaler (HPA) resource between versions 1.24 and 1.27.\n","permalink":"https://okulbida.com/posts/helm-fix-after-k8s-upgrade/","summary":"After upgrading Kubernetes (k8s), you might encounter errors such as no matches for kind \u0026quot;Deployment\u0026quot; in version \u0026quot;apps/v1beta1\u0026quot;. These errors typically indicate that certain resources have become deprecated. To resolve these issues without the need to delete your Helm chart, you can follow this simple solution:\nhelm plugin install https://github.com/helm/helm-mapkubeapis helm mapkubeapis \u0026lt;releasename\u0026gt; helm upgrade \u0026lt;releasename\u0026gt; It\u0026rsquo;s important to note that you may still need to update your Helm chart templates, especially if there have been structural changes between versions.","title":"Resolving Helm issues after kubernetes upgrade"},{"content":"Expose Amazon EKS pods through cross-account load balancer\n https://aws.amazon.com/blogs/containers/expose-amazon-eks-pods-through-cross-account-load-balancer/  ","permalink":"https://okulbida.com/posts/eks-expose-pods-through-cross-account-lb/","summary":"Expose Amazon EKS pods through cross-account load balancer\n https://aws.amazon.com/blogs/containers/expose-amazon-eks-pods-through-cross-account-load-balancer/  ","title":"EKS expose pods through cross-account Load balancer"},{"content":"Simplified EKS access\n https://aws.amazon.com/about-aws/whats-new/2023/12/amazon-eks-controls-iam-cluster-access-management/ https://aws.amazon.com/blogs/containers/a-deep-dive-into-simplified-amazon-eks-access-management-controls/ https://github.com/hashicorp/terraform-provider-aws/issues/34982  ","permalink":"https://okulbida.com/posts/eks-access/","summary":"Simplified EKS access\n https://aws.amazon.com/about-aws/whats-new/2023/12/amazon-eks-controls-iam-cluster-access-management/ https://aws.amazon.com/blogs/containers/a-deep-dive-into-simplified-amazon-eks-access-management-controls/ https://github.com/hashicorp/terraform-provider-aws/issues/34982  ","title":"EKS simplified access"},{"content":"While using Loki with S3 and Dynamodb it\u0026rsquo;s mandatory to add provision_config details as default might affect your budget https://grafana.com/docs/loki/latest/configuration/#provision_config\n[provisioned_write_throughput: \u0026lt;int\u0026gt; | default = 3000] # DynamoDB table default read throughput. # CLI flag: -\u0026lt;prefix\u0026gt;.read-throughput [provisioned_read_throughput: \u0026lt;int\u0026gt; | default = 300] ","permalink":"https://okulbida.com/posts/loki-s3-dynamodb/","summary":"While using Loki with S3 and Dynamodb it\u0026rsquo;s mandatory to add provision_config details as default might affect your budget https://grafana.com/docs/loki/latest/configuration/#provision_config\n[provisioned_write_throughput: \u0026lt;int\u0026gt; | default = 3000] # DynamoDB table default read throughput. # CLI flag: -\u0026lt;prefix\u0026gt;.read-throughput [provisioned_read_throughput: \u0026lt;int\u0026gt; | default = 300] ","title":"Loki S3 dynamodb"},{"content":"","permalink":"https://okulbida.com/posts/bigdata-comparison-cloudproviders/","summary":"","title":"Bigdata comparison within AWS,Azure,GCP"},{"content":"","permalink":"https://okulbida.com/posts/db-cloudproviders/","summary":"","title":"Database comparison within AWS,Azure,GCP"},{"content":"You can now launch NAT Gateways in your VPC without associating an internet gateway to your VPC. Internet Gateway is required to provide internet access to the NAT Gateway. However, some customers use their NAT Gateways with Transit Gateway or virtual private gateway to communicate privately with other VPCs or on-premises environments and thus, do not need an internet gateway attached to their VPCs.\nMore details: https://aws.amazon.com/about-aws/whats-new/2021/06/aws-removes-nat-gateways-dependence-on-internet-gateway-for-private-communications/\n","permalink":"https://okulbida.com/posts/aws-nat-gateway-no-need-igw/","summary":"You can now launch NAT Gateways in your VPC without associating an internet gateway to your VPC. Internet Gateway is required to provide internet access to the NAT Gateway. However, some customers use their NAT Gateways with Transit Gateway or virtual private gateway to communicate privately with other VPCs or on-premises environments and thus, do not need an internet gateway attached to their VPCs.\nMore details: https://aws.amazon.com/about-aws/whats-new/2021/06/aws-removes-nat-gateways-dependence-on-internet-gateway-for-private-communications/","title":"AWS removes NAT Gateway’s dependence on Internet Gateway for Private communications"},{"content":"What’s new in Grafana v8.0\n  Grafana includes built-in support for Prometheus Alertmanager. Once you add it as a data source, you can use the Grafana alerting UI to manage silences, contact points as well as notification policies. A drop down option in these pages allows you to switch between Grafana and any configured Alertmanager data sources. https://grafana.com/docs/grafana/latest/datasources/alertmanager/\n  Prometheus metrics browser https://grafana.com/docs/grafana/latest/datasources/prometheus/#metrics-browser\n  More details: https://grafana.com/docs/grafana/latest/whatsnew/whats-new-in-v8-0/\n","permalink":"https://okulbida.com/posts/grafana-8-released/","summary":"What’s new in Grafana v8.0\n  Grafana includes built-in support for Prometheus Alertmanager. Once you add it as a data source, you can use the Grafana alerting UI to manage silences, contact points as well as notification policies. A drop down option in these pages allows you to switch between Grafana and any configured Alertmanager data sources. https://grafana.com/docs/grafana/latest/datasources/alertmanager/\n  Prometheus metrics browser https://grafana.com/docs/grafana/latest/datasources/prometheus/#metrics-browser\n  More details: https://grafana.com/docs/grafana/latest/whatsnew/whats-new-in-v8-0/","title":"What’s new in Grafana v8.0"},{"content":"https://aws.amazon.com/about-aws/whats-new/2021/05/aws-load-balancer-controller-version-2-2-available-support-nlb-instance/\n","permalink":"https://okulbida.com/posts/aws-lb-controller-nlb-support/","summary":"https://aws.amazon.com/about-aws/whats-new/2021/05/aws-load-balancer-controller-version-2-2-available-support-nlb-instance/","title":"AWS Load Balancer Controller version 2.2 now available with support for NLB instance targeting"},{"content":"Amazon EC2 Auto Scaling now natively supports Predictive Scaling so you can proactively scale out your Auto Scaling group to be ready for upcoming demand. Predictive Scaling can help you avoid the need to over-provision capacity, resulting in lower EC2 cost, while ensuring your application’s responsiveness. (Previously, Predictive Scaling was only available via AWS Auto Scaling Plans.)\nhttps://aws.amazon.com/about-aws/whats-new/2021/05/amazon-ec2-auto-scaling-introduces-predictive-scaling-native-scaling-policy/\n","permalink":"https://okulbida.com/posts/aws-ec2-predictive-autoscaling/","summary":"Amazon EC2 Auto Scaling now natively supports Predictive Scaling so you can proactively scale out your Auto Scaling group to be ready for upcoming demand. Predictive Scaling can help you avoid the need to over-provision capacity, resulting in lower EC2 cost, while ensuring your application’s responsiveness. (Previously, Predictive Scaling was only available via AWS Auto Scaling Plans.)\nhttps://aws.amazon.com/about-aws/whats-new/2021/05/amazon-ec2-auto-scaling-introduces-predictive-scaling-native-scaling-policy/","title":"Amazon EC2 Auto Scaling Introduces Predictive Scaling as a Native Scaling Policy"},{"content":"Amazon Elastic Kubernetes Service (Amazon EKS) now supports using the Amazon EKS console, CLI, and API to install and manage CoreDNS and kube-proxy in addition to existing support for the Amazon VPC CNI networking plugin.\nhttps://aws.amazon.com/about-aws/whats-new/2021/05/eks-add-ons-now-support-coredns-kube-proxy/\nhttps://docs.aws.amazon.com/eks/latest/userguide/eks-add-ons.html\n","permalink":"https://okulbida.com/posts/awsekscoredns/","summary":"Amazon Elastic Kubernetes Service (Amazon EKS) now supports using the Amazon EKS console, CLI, and API to install and manage CoreDNS and kube-proxy in addition to existing support for the Amazon VPC CNI networking plugin.\nhttps://aws.amazon.com/about-aws/whats-new/2021/05/eks-add-ons-now-support-coredns-kube-proxy/\nhttps://docs.aws.amazon.com/eks/latest/userguide/eks-add-ons.html","title":"EKS Add-Ons Now Supports CoreDNS and kube-proxy"},{"content":"Full list of videos from KubeCon 2021 Europe\nhttps://www.youtube.com/playlist?list=PLj6h78yzYM2MqBm19mRz9SYLsw4kfQBrC\n","permalink":"https://okulbida.com/posts/kubecon2021/","summary":"Full list of videos from KubeCon 2021 Europe\nhttps://www.youtube.com/playlist?list=PLj6h78yzYM2MqBm19mRz9SYLsw4kfQBrC","title":"Kubecon2021"},{"content":"AWS CloudFront functions is a nice alternative to Lambda@Edge\nhttps://aws.amazon.com/blogs/aws/introducing-cloudfront-functions-run-your-code-at-the-edge-with-low-latency-at-any-scale/\n","permalink":"https://okulbida.com/posts/aws-cloudfront-functions/","summary":"AWS CloudFront functions is a nice alternative to Lambda@Edge\nhttps://aws.amazon.com/blogs/aws/introducing-cloudfront-functions-run-your-code-at-the-edge-with-low-latency-at-any-scale/","title":"AWS CloudFront functions"},{"content":"Amazon EC2 enables you to replace the root EBS volume for a running instance\nLimitations:  You can\u0026rsquo;t replace the root volume if it is an instance store volume. You can\u0026rsquo;t replace the root volume for metal instances.  More details: https://aws.amazon.com/about-aws/whats-new/2021/04/ec2-enables-replacing-root-volumes-for-quick-restoration-and-troubleshooting/\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-restoring-volume.html#replace-root\n","permalink":"https://okulbida.com/posts/ec2-root-volume-replacing/","summary":"Amazon EC2 enables you to replace the root EBS volume for a running instance\nLimitations:  You can\u0026rsquo;t replace the root volume if it is an instance store volume. You can\u0026rsquo;t replace the root volume for metal instances.  More details: https://aws.amazon.com/about-aws/whats-new/2021/04/ec2-enables-replacing-root-volumes-for-quick-restoration-and-troubleshooting/\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-restoring-volume.html#replace-root","title":"EC2 root volume replacing"},{"content":"https://y0zg.github.io/public-blog/pf-infographic-promql-cheatsheet.pdf\n","permalink":"https://okulbida.com/posts/prom-cheat-sheet/","summary":"https://y0zg.github.io/public-blog/pf-infographic-promql-cheatsheet.pdf","title":"PromQL cheat sheet"},{"content":"How to use AWS Secrets \u0026amp; Configuration Provider with your Kubernetes Secrets Store CSI driver. One more example of kubernetes secrets management among vault, external-secrets and 1password operator 😅\nhttps://aws.amazon.com/blogs/security/how-to-use-aws-secrets-configuration-provider-with-kubernetes-secrets-store-csi-driver/\n","permalink":"https://okulbida.com/posts/aws-secrets-csi/","summary":"How to use AWS Secrets \u0026amp; Configuration Provider with your Kubernetes Secrets Store CSI driver. One more example of kubernetes secrets management among vault, external-secrets and 1password operator 😅\nhttps://aws.amazon.com/blogs/security/how-to-use-aws-secrets-configuration-provider-with-kubernetes-secrets-store-csi-driver/","title":"AWS Secrets CSI for EKS"},{"content":"Grafana was relicensed to AGPLv3\nhttps://grafana.com/blog/2021/04/20/grafana-loki-tempo-relicensing-to-agplv3/\n","permalink":"https://okulbida.com/posts/grafana-license/","summary":"Grafana was relicensed to AGPLv3\nhttps://grafana.com/blog/2021/04/20/grafana-loki-tempo-relicensing-to-agplv3/","title":"Grafana License"},{"content":"","permalink":"https://okulbida.com/categories/","summary":"","title":""},{"content":"Oleksandr Kulbida is a seasoned cloud engineer with focus on opensource. His passions are around breaking silos between teams and automating everything! Oleksandr follows Unix philosophy and loves adopting infrastructure as code. Feel free to connect on LinkedIn https://www.linkedin.com/in/oleksandrkulbida/ ","permalink":"https://okulbida.com/about/","summary":"about","title":"About"},{"content":"","permalink":"https://okulbida.com/archive/","summary":"archive","title":"Archive"}]