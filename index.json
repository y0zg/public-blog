[{"content":"This list is being updated on regular basis\nGeneral info  https://kubernetesreadme.com/  Comparison  API comparison https://kube-api.ninja/ https://learnk8s.io/research https://docs.google.com/spreadsheets/d/1RPpyDOLFmcgxMCpABDzrsBYWpPYCIBuvAoUQLwOGoQw/edit#gid=907731238  Hosting  kapsule https://www.scaleway.com/en/ free https://cloud.okteto.com/  Secrets  External secrets management integration with k8s https://github.com/godaddy/kubernetes-external-secrets Integrate Kubernetes with 1Password https://github.com/1Password/onepassword-operator Kubernetes mutating webhook for secrets-init injection https://github.com/doitintl/kube-secrets-init AWS EKS Secrets store CSI driver https://aws.amazon.com/blogs/security/how-to-use-aws-secrets-configuration-provider-with-kubernetes-secrets-store-csi-driver/  RBAC  RBAC practices and tooling https://rbac.dev/ Visualize RBAC https://github.com/team-soteria/rback RBAC Manager is designed to simplify authorization in Kubernetes https://github.com/FairwindsOps/rbac-manager Access matrix https://github.com/corneliusweig/rakkess  Security  Kubernetes vector attack https://github.com/cyberark/kubesploit  https://github.com/cyberark/kubesploit/blob/assets/mitre_pic_full.png    Production checklist  https://learnk8s.io/production-best-practices/  Deployment Helm  https://v3.helm.sh/docs/howto/charts_tips_and_tricks  Serverless  Knative https://knative.dev/docs/eventing/sources/ Kubeless https://kubeless.io/ OpenFAAS https://github.com/openfaas/faas  Local  Minikube https://minikube.sigs.k8s.io/ https://docs.tilt.dev/ k0s https://github.com/k0sproject/k0s Microk8s from Canonical https://microk8s.io/ KIND https://kind.sigs.k8s.io/ Telepresence https://www.telepresence.io/tutorials/kubernetes-rapid Exposes your local resources to kubernetes (like telepresence) https://github.com/omrikiei/ktunnel Skaffold https://skaffold.dev/  Capacity  Overview of the resource requests, limits, and utilization in a Kubernetes https://github.com/robscott/kube-capacity Recommendations requests/limits https://github.com/robusta-dev/krr  K8s Plugins  Plugin installer https://github.com/kubernetes-sigs/krew  Plugins list https://krew.sigs.k8s.io/plugins   Debug pods https://github.com/aylei/kubectl-debug Resources https://github.com/y0zg/kubectl-resources View webhook https://github.com/Trendyol/kubectl-view-webhook#kubectl-view-webhook access-matrix - show an access matrix for k8s server resources https://github.com/corneliusweig/rakkess rbac-lookup - Easily find roles and cluster roles attached to any user, service account, or group name in your Kubernetes cluster https://github.com/FairwindsOps/rbac-lookup rbac-view - Visualize Kubernetes RBAC rules https://github.com/jasonrichardsmith/rbac-view pv-df - Show disk usage (like unix df) for persistent volumes https://artifacthub.io/packages/krew/krew-index/df-pv resource-quotas sniff - tcpdump pods https://github.com/eldadru/ksniff view-secret - decode Kubernetes secrets https://github.com/elsesiy/kubectl-view-secret Exposes your local resources to kubernetes https://github.com/omrikiei/ktunnel kubectl git blame https://github.com/knight42/kubectl-blame  Upgrade  Detect deprecated resources https://github.com/FairwindsOps/pluto Fix helm chart after k8s upgrade helm plugin install https://github.com/helm/helm-mapkubeapis  Baremetal  Awesome baremetal https://github.com/alexellis/awesome-baremetal EKS Anywhere https://aws.amazon.com/eks/eks-anywhere/ GKE Anthos https://cloud.google.com/anthos/clusters Popular stack options:  Kubeadm+Flannel+Linstor+MetalLB kubeadm + terraform kubeadm CNI calico (MaaS) + Terraform + RKE RKE + terraform VIP for pods https://kube-vip.io/ vmware+kismatic+ansible CNI calico vmware PKS    Security/Firewall  Set up roles in IAM, map them to K8s groups, write RBAC bindings against those groups https://github.com/kubernetes-sigs/aws-iam-authenticator#full-configuration-format https://github.com/gravitational/wormhole Calico network policy strongswan vpn istio envoyfilters OPA  Kyverno is a policy engine designed for Kubernetes. Based on the Open Policy Agent https://kyverno.io/ cases  Sync secrets https://kyverno.io/policies/other/sync_secrets/?policytypes=Secret Disallow Secrets from Env     Vectors attack - https://github.com/cyberark/kubesploit Kubesploit https://github.com/cyberark/kubesploit/blob/assets/mitre_pic_full.png Intentionally vulnerable cluster environment to learn and practice Kubernetes security https://github.com/madhuakula/kubernetes-goat kubectl-dig - Deep Kubernetes visibility from the kubectl https://github.com/sysdiglabs/kubectl-dig Realoader configmaps and secrets https://github.com/stakater/Reloader  https://dev.to/joshduffney/kubernetes-using-configmap-subpaths-to-mount-files-3a1i    Registry  Cache images between nodes https://github.com/XenitAB/spegel Container Registry and Image Management for Kubernetes Clusters https://github.com/ContainerSolutions/trow Sync registries https://github.com/plexsystems/sinker  Chaos testing  https://chaos-mesh.org/ https://github.com/berkay-dincer/kubethanos  K8s at home  https://github.com/eddiezane/kubecon-eu-2021-automating-your-home-with-k3s-and-home-assistant-notes/blob/main/README.md https://github.com/k8s-at-home https://github.com/eddiezane/pikube https://github.com/k8s-at-home/template-cluster-k3s https://www.reddit.com/r/homelab/  ","permalink":"https://okulbida.com/posts/kubernetes_tools/","summary":"This list is being updated on regular basis\nGeneral info  https://kubernetesreadme.com/  Comparison  API comparison https://kube-api.ninja/ https://learnk8s.io/research https://docs.google.com/spreadsheets/d/1RPpyDOLFmcgxMCpABDzrsBYWpPYCIBuvAoUQLwOGoQw/edit#gid=907731238  Hosting  kapsule https://www.scaleway.com/en/ free https://cloud.okteto.com/  Secrets  External secrets management integration with k8s https://github.com/godaddy/kubernetes-external-secrets Integrate Kubernetes with 1Password https://github.com/1Password/onepassword-operator Kubernetes mutating webhook for secrets-init injection https://github.com/doitintl/kube-secrets-init AWS EKS Secrets store CSI driver https://aws.amazon.com/blogs/security/how-to-use-aws-secrets-configuration-provider-with-kubernetes-secrets-store-csi-driver/  RBAC  RBAC practices and tooling https://rbac.dev/ Visualize RBAC https://github.com/team-soteria/rback RBAC Manager is designed to simplify authorization in Kubernetes https://github.","title":"Kubernetes tools"},{"content":"  Keep the cloud provider platform secure\n Least privilege Secure traffic into cluster    Run security test in development environments\n Mirror environments    Cluster authentication \u0026amp; authorization\n Leverage OIDC for k8s authentication RBAC - define roles    In cluster network/security/micro segmentation\n Prevent namespace-to-namespace communication Network policy    Policy \u0026amp; Governance\n  k8s admission controllers\n Read only Non-privileges ports    Runtime security \u0026amp; monitoring\n Identity security privilege not needed Track anomalies Falco \u0026amp; Twistlock    Application secret management\n Encrypt Vault Sealed secrets SOPS    Data protection \u0026amp; CVE remediation\n Approved API versions Review release notes Restric ingress/egress Allow approved docker images    Auditing\n Define policies Trigger alerts Store audit logs    Container security\n Vulnerability scanning in pipeline MTLS - adhere to k8s pod security standards    ","permalink":"https://okulbida.com/posts/k8s-security-best-practices/","summary":"Keep the cloud provider platform secure\n Least privilege Secure traffic into cluster    Run security test in development environments\n Mirror environments    Cluster authentication \u0026amp; authorization\n Leverage OIDC for k8s authentication RBAC - define roles    In cluster network/security/micro segmentation\n Prevent namespace-to-namespace communication Network policy    Policy \u0026amp; Governance\n  k8s admission controllers\n Read only Non-privileges ports    Runtime security \u0026amp; monitoring","title":"k8s security best practices"},{"content":"Disclaimer: here you might not find something new if you know 12 factors app.\nThe 12-factor app methodology 12factor.net, is a set of principles designed to enable applications to be built with portability and resilience when deployed to the web. These principles focus on declarative formats for automation, clean contracts with the operating system, and suitability for deployment on modern cloud platforms, thus minimizing divergence between development and production, enabling continuous deployment for maximum agility.\nHere\u0026rsquo;s how each factor is applied and the benefits they bring, drawn from firsthand experiences in the field:\n1. Codebase Use Git repositories to store your application code. Different branches can align with different deployments through CI/CD pipelines.\n2. Dependencies Explicitly declare and isolate dependencies: Managing dependencies via a declaration document (package.json) and ensuring no implicit reliance on system-wide packages ensures that the app runs consistently across all environments.\nIn k8s, containerization usage can encapsulate environments perfectly to maintain this isolation.\n3. Config Store config in the environment: Configuration variables (like database URLs and external service credentials) should be stored in the environment to keep configuration dynamic and secure. This separation of config from code helps in avoiding security risks and making the app environment agnostic, simplifying deployment across different environments.\nIn k8s, utilize ConfigMaps, Secrets, mutation webhook to store and manage environmental configurations separately from the application code. This keeps sensitive information out of your codebase. Your sensitive data should be preferrably injected into memory or mounted against added as env variable.\n4. Backing services Treat backing services as attached resources: Services like databases, queueing systems, and caching systems should be treated as attachable resources, which can be replaced or attached without code changes. This makes scaling and migrating to different services easier without major codebase changes.\n5. Build, release, run CI/CD pipelines should handle the build and deployment stages separately, ensuring that the build stage produces an immutable artifact that moves to the release and run stages.\n6. Processes Execute the app as one or more stateless processes: State should be externalized (e.g., using a database or caching layer), allowing the application to scale horizontally without side-effects. Stateless applications are easier to manage and scale, as each process can be started or stopped independently.\nThis also aligns with Unix philosophy and CNCF principle where systems must have clear separation between their processes.\nCNCF principal:\nCloud native systems have a clear separation between their processes [2]. They utilize the Unix philosophy of doing one thing and doing it well. These microservices usually use technologies such as containers and aim for one process per container [3]. As such, cloud native applications should have all of their dependencies packaged in the container during the build phase and leveraged during deployment [4]. In k8s world we don\u0026rsquo;t store pod state internally. Use external data stores like database or cache to manage state.\n7. Port binding Containers within pods expose ports which are then mapped to services that abstract these details away, allowing external traffic to reach the containers.\n8. Concurrency Use k8s HPA to manage the scaling of applications based on CPU usage or other metrics, effectively handling concurrency.\n9. Disposability If your application doesn‚Äôt gracefully shut down when receiving a SIGTERM you can use kubernetes lifecycle preStop hook to trigger a graceful shutdown. You can set terminationGracePeriodSeconds to change graceful shutdown time to ensure SIGKILL isn\u0026rsquo;t sent forcibly.\n10. Dev/prod parity Maintain similar configurations across environments using Kubernetes manifests or Helm charts values, minimizing drift between environments.\n11. Logs Implement logging at the application level, exporting logs to stdout and preferrably in JSON format. Kubernetes then aggregates these logs, which can be collected and analyzed by logs collectors like Logstash, Fluentd, etc.\n12. Admin processes Use k8s Job for one-off tasks like database migrations or batch jobs. This ensures that these tasks run in an environment identical to the regular application environment.\nWhich factors do you think are missing in 12 factors but present in k8s? ;)\nIn addition to the traditional 12 factors, there are more principles considered crucial for modern production environments, especially in cloud platforms like Kubernetes.\n13.Observability Ensures applications provide insights into their internal states and metrics, crucial for managing distributed systems effectively. Traces, metrics, profiling are very useful and loved by developers.\n14. Schedulability Focuses on predictable resource demands, enabling better scheduling and stability across distributed services. K8s requests, limits are your good friends, specially if you are on lower versions of k8s.\n15. Upgradability Pertains to easy and seamless updates and data format evolution without downtime, maintaining forward compatibility. Here we should mention such upgrades like rolling, blue-green, canary, progressive delivery and more.\n16. Least privilege Involves running processes with the minimal set of permissions necessary, enhancing security\n17. Auditability Allows tracking of who did what and when, which is essential for security and compliance\n18. Security Concerns the hardening of applications against unauthorized access and attacks. Usage of tools like OPA, Kyverno, Falso is a good start.\n19. Measurability Involves quantifying resource usage and application performance, important for cost management and scaling decisions. Tools like Anodot or kubecost are very cool for understanding your bills.\nOther useful resources:\n SRE Principles SRE lessons learned CAP theorem AWS Well-architechted framework CNCF principles  ","permalink":"https://okulbida.com/posts/12-factors/","summary":"Disclaimer: here you might not find something new if you know 12 factors app.\nThe 12-factor app methodology 12factor.net, is a set of principles designed to enable applications to be built with portability and resilience when deployed to the web. These principles focus on declarative formats for automation, clean contracts with the operating system, and suitability for deployment on modern cloud platforms, thus minimizing divergence between development and production, enabling continuous deployment for maximum agility.","title":"12 Factors vs kubernetes world"},{"content":"Recent elasticsearch licensing change ensures that the Beats modules are sending data to an officially supported versions of Elasticsearch and Kibana where Elastic can attest to the quality and scale of the products. Does AWS have any plans to fork a version filebeat?\nhttps://www.elastic.co/guide/en/beats/libbeat/current/breaking-changes-7.13.html\nhttps://www.reddit.com/r/aws/comments/nn95aq/elastic_has_broken_filebeat_as_of_713_it_no/\nWhat are the alternatives?\n Host elasticsearch on EC2 instances, why not? CloudWatch, slow\u0026hellip; https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/CWL_ES_Stream.html Kubernetes options like ECK or helm chart deployments\u0026hellip;tricky for production usage Loki? Is it mature enough?  UPDATE 2021-07-12: OpenSearch 1.0 launches with Apache License, Version 2.0 and AWS huge contribution efforts\nhttps://aws.amazon.com/blogs/opensource/opensearch-1-0-launches/\nUPDATE 2024-04-21:  Quickwit - very interesting alternative for common logging tools written on rust Elasticsearch can be deployed with helm chart but based on experience you should consider using single-AZ. This might relevant to logs which are not as critical as business data. Loki is pretty good solution, it integrates well with Grafana Tempo for tracing purposes and Opentelemetry. But based on my experience, developers for some reasons don\u0026rsquo;t love search capabilities in Grafana Loki as it\u0026rsquo;s available in kibana or other commercial logging solutions like datadog.  ","permalink":"https://okulbida.com/posts/aws-elasticsearch-licensing/","summary":"Recent elasticsearch licensing change ensures that the Beats modules are sending data to an officially supported versions of Elasticsearch and Kibana where Elastic can attest to the quality and scale of the products. Does AWS have any plans to fork a version filebeat?\nhttps://www.elastic.co/guide/en/beats/libbeat/current/breaking-changes-7.13.html\nhttps://www.reddit.com/r/aws/comments/nn95aq/elastic_has_broken_filebeat_as_of_713_it_no/\nWhat are the alternatives?\n Host elasticsearch on EC2 instances, why not? CloudWatch, slow\u0026hellip; https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/CWL_ES_Stream.html Kubernetes options like ECK or helm chart deployments\u0026hellip;tricky for production usage Loki?","title":"Aws vs Elasticsearch licensing"},{"content":"Reasons to upgrade to k8s 1.30 Container resource based autoscaling Container resource based autoscaling is now promoted to stable https://github.com/kubernetes/enhancements/issues/1610 Horizontal Pod Autoscaler examines the total resource usage of the entire pod i.e. sum of all containers and scale pods based on average CPU or memory usage. Container resource based autoscaling feature allows HPA to scale workloads based on the resource usage of individual containers within a pod, instead of the aggregated usage of all containers in the pod\nAggregated Discovery If you use Helm \u0026amp; Flux, you should consider upgrading to Kubernetes 1.30 which now offers GA Aggregated Discovery. This is particularly useful on clusters with many CRDs, where the number of API calls Helm SDK will decrease substantially.\n","permalink":"https://okulbida.com/posts/k8s-130-version/","summary":"Reasons to upgrade to k8s 1.30 Container resource based autoscaling Container resource based autoscaling is now promoted to stable https://github.com/kubernetes/enhancements/issues/1610 Horizontal Pod Autoscaler examines the total resource usage of the entire pod i.e. sum of all containers and scale pods based on average CPU or memory usage. Container resource based autoscaling feature allows HPA to scale workloads based on the resource usage of individual containers within a pod, instead of the aggregated usage of all containers in the pod","title":"k8s 1.30 version"},{"content":"Kubernetes InPlacePodVerticalScaling feature\nKubernetes v1.27 introduces InPlacePodVerticalScaling, allowing seamless pod resource resizing without restarts\nEnhanced Continuity: Eliminates the downtime and potential data loss caused by pod restart\nCost Savings: Avoid overprovisioning and optimizing resource usage. InPlacePodVerticalScaling lets you allocate resources precisely as needed\nIn this example for pod memory resources configuration, the resizePolicy indicates that changes to the memory allocation require a restart of the container, and for CPU resources the restart is not necessary during resizing.\ncat \u0026lt;\u0026lt;EOF | kubectl apply -f - --- apiVersion: v1 kind: Pod metadata: labels: run: nginx name: nginx spec: containers: - image: nginx name: nginx resizePolicy: - resourceName: \u0026#34;memory\u0026#34; restartPolicy: \u0026#34;RestartContainer\u0026#34; - resourceName: \u0026#34;cpu\u0026#34; restartPolicy: \u0026#34;NotRequired\u0026#34; resources: limits: cpu: \u0026#34;300m\u0026#34; memory: \u0026#34;1Gi\u0026#34; requests: cpu: \u0026#34;100m\u0026#34; memory: \u0026#34;500Mi\u0026#34; EOF ","permalink":"https://okulbida.com/posts/k8s-new-features/","summary":"Kubernetes InPlacePodVerticalScaling feature\nKubernetes v1.27 introduces InPlacePodVerticalScaling, allowing seamless pod resource resizing without restarts\nEnhanced Continuity: Eliminates the downtime and potential data loss caused by pod restart\nCost Savings: Avoid overprovisioning and optimizing resource usage. InPlacePodVerticalScaling lets you allocate resources precisely as needed\nIn this example for pod memory resources configuration, the resizePolicy indicates that changes to the memory allocation require a restart of the container, and for CPU resources the restart is not necessary during resizing.","title":"k8s InPlacePodVerticalScaling"},{"content":"Collaboration  Use remote state and state locking  For certain backends like AWS S3, enable versioning to make it easier to recover your state if needed   Agree on naming convention Use meaningful tags to easily identify resources: environment, owner, project keys are must  You can also add cloud-custdodian for components which are out of terrarfom/IaC tools, which could automatically tag your manually created resources with Owner Creator based on CloudTrail events    Don\u0026rsquo;t reinvent the wheel Use existing shared and community modules. As a common sense, it\u0026rsquo;s highly recommended to reuse matured modules such as VPC. Look for these modules in Terraform Registry\nExplicit definition  Keep your providers, modules versioned properly Keep each module in a separate repo. Usually it depends on project size, and we can use monorepo or single modules repo as well.  Avoid variables hard-coding Check if you can get the value of an attribute via a data source instead of setting it explicitly. For example, instead of finding our AWS account id from the console and setting it in terraform.tfvars as\naws_account_id=‚Äù99999999999‚Äù we can get it from a data source\ndata \u0026quot;aws_caller_identity\u0026quot; \u0026quot;current\u0026quot; {} locals { account_id = data.aws_caller_identity.current.account_id } Automate   Use pre-commit https://pre-commit.com/#install https://github.com/antonbabenko/pre-commit-terraform\n  Must have hooks:\n terraform_fmt terraform_validate terraform_docs terraform_tflint checkov    DRY (Don\u0026rsquo;t repeat yourself)\n Consider using terragrunt if you need advanced dependency management. It\u0026rsquo;s also suitable if you need advanced dependency management and want to simplify remote state management    CICD\n For PRs collaboration use Atlantis For infrastructure drifts detection use https://github.com/snyk/driftctl    ","permalink":"https://okulbida.com/posts/terraform-best-practices/","summary":"Collaboration  Use remote state and state locking  For certain backends like AWS S3, enable versioning to make it easier to recover your state if needed   Agree on naming convention Use meaningful tags to easily identify resources: environment, owner, project keys are must  You can also add cloud-custdodian for components which are out of terrarfom/IaC tools, which could automatically tag your manually created resources with Owner Creator based on CloudTrail events    Don\u0026rsquo;t reinvent the wheel Use existing shared and community modules.","title":"Terraform best practices"},{"content":"Userdata is compatible with the standard AWS EKS Terraform module, with the sole recommendation being the utilization of a custom AMI. In order to use instance-store you also need to install local-static-provisioner - https://github.com/kubernetes-sigs/sig-storage-local-static-provisioner\nTerraform example:\neks-dev-instance-store = { instance_types = [\u0026#34;r6id.large\u0026#34;] min_size = 1 max_size = 3 desired_size = 1 block_device_mappings = {# Root volume  xvda = { device_name = \u0026#34;/dev/xvda\u0026#34; ebs = { volume_size = 24 volume_type = \u0026#34;gp3\u0026#34; iops = 3000 encrypted = false delete_on_termination = true } } } ami_id = data.aws_ami.ubuntu.image_id# The virtual device name (ephemeralN). Instance store volumes are numbered # starting from 0. An instance type with 2 available instance store volumes # can specify mappings for ephemeral0 and ephemeral1. The number of available # instance store volumes depends on the instance type. After you connect to # the instance, you must mount the volume - here, we are using user data to automatically # mount the volume(s) during instance creation. # # NVMe instance store volumes are automatically enumerated and assigned a device # name. Including them in your block device mapping has no effect. # post_bootstrap_user_data  enable_bootstrap_user_data = true# NVMe instance store volumes are automatically enumerated and assigned a device  pre_bootstrap_user_data = \u0026lt;\u0026lt;-EOT echo \u0026#34;Running a custom user data script\u0026#34; set -ex apt-get update apt-get install -y nvme-cli mdadm xfsprogs# Fetch the list of NVMe devices  DEVICES=$(lsblk -d -o NAME | grep nvme) DISK_ARRAY=() for DEV in $DEVICES do# Exclude the root disk, /dev/nvme0n1, from the list of devices  if [[ $${DEV} != \u0026#34;nvme0n1\u0026#34; ]]; then NVME_INFO=$(nvme id-ctrl --raw-binary \u0026#34;/dev/$${DEV}\u0026#34; | cut -c3073-3104 | tr -s \u0026#39; \u0026#39; | sed \u0026#39;s/ $//g\u0026#39;)# Check if the device is Amazon EC2 NVMe Instance Storage  if [[ $${NVME_INFO} == *\u0026#34;ephemeral\u0026#34;* ]]; then DISK_ARRAY+=(\u0026#34;/dev/$${DEV}\u0026#34;) fi fi done DISK_COUNT=$${#DISK_ARRAY[@]} if [ $${DISK_COUNT} -eq 0 ]; then echo \u0026#34;No NVMe SSD disks available. No further action needed.\u0026#34; else if [ $${DISK_COUNT} -eq 1 ]; then TARGET_DEV=$${DISK_ARRAY[0]} mkfs.xfs $${TARGET_DEV} else mdadm --create --verbose /dev/md0 --level=0 --raid-devices=$${DISK_COUNT} $${DISK_ARRAY[@]} mkfs.xfs /dev/md0 TARGET_DEV=/dev/md0 fi mkdir -p /local1 echo $${TARGET_DEV} /local1 xfs defaults,noatime 1 2 \u0026gt;\u0026gt; /etc/fstab mount -a /usr/bin/chown -hR +999:+1000 /local1 fi EOT labels = { group = \u0026#34;instance-store\u0026#34; } taints = { dedicated = { key = \u0026#34;group\u0026#34; value = \u0026#34;instance-store\u0026#34; effect = \u0026#34;NO_SCHEDULE\u0026#34; } } update_config = { max_unavailable_percentage = 25 } tags = { ExtraTag = \u0026#34;instance-store\u0026#34; \u0026#34;k8s.io/cluster-autoscaler/enabled\u0026#34; = \u0026#34;true\u0026#34; \u0026#34;k8s.io/cluster-autoscaler/${local.name}\u0026#34; = \u0026#34;owned\u0026#34; } } ","permalink":"https://okulbida.com/posts/eks-instance-store/","summary":"Userdata is compatible with the standard AWS EKS Terraform module, with the sole recommendation being the utilization of a custom AMI. In order to use instance-store you also need to install local-static-provisioner - https://github.com/kubernetes-sigs/sig-storage-local-static-provisioner\nTerraform example:\neks-dev-instance-store = { instance_types = [\u0026#34;r6id.large\u0026#34;] min_size = 1 max_size = 3 desired_size = 1 block_device_mappings = {# Root volume  xvda = { device_name = \u0026#34;/dev/xvda\u0026#34; ebs = { volume_size = 24 volume_type = \u0026#34;gp3\u0026#34; iops = 3000 encrypted = false delete_on_termination = true } } } ami_id = data.","title":"EKS with instance-store nitro-based node-group"},{"content":"There are numerous solutions for accessing private RDS instances, many of which require thoughtful design. The solution I use sometimes is straightforward: I deploy it as a Helm chart within a k8s cluster. In this setup, access to the RDS is contingent on having access to the k8s cluster with the appropriate RBAC configurations. While it may not be perfect, it\u0026rsquo;s secure, quick to implement, and requires minimal maintenance. The following command demonstrates the basic principle:\nsocat TCP4-LISTEN:8888,fork TCP4:xxxxxxxx.us-east-1.rds.amazonaws.com:5432 We execute `socat`` command on an instance or pod. Subsequently, we need to forward the port to our local machine\n","permalink":"https://okulbida.com/posts/simple-rds-access/","summary":"There are numerous solutions for accessing private RDS instances, many of which require thoughtful design. The solution I use sometimes is straightforward: I deploy it as a Helm chart within a k8s cluster. In this setup, access to the RDS is contingent on having access to the k8s cluster with the appropriate RBAC configurations. While it may not be perfect, it\u0026rsquo;s secure, quick to implement, and requires minimal maintenance. The following command demonstrates the basic principle:","title":"Simple rds access"},{"content":"After upgrading Kubernetes (k8s), you might encounter errors such as no matches for kind \u0026quot;Deployment\u0026quot; in version \u0026quot;apps/v1beta1\u0026quot;. These errors typically indicate that certain resources have become deprecated. To resolve these issues without the need to delete your Helm chart, you can follow this simple solution:\nhelm plugin install https://github.com/helm/helm-mapkubeapis helm mapkubeapis \u0026lt;releasename\u0026gt; helm upgrade \u0026lt;releasename\u0026gt; It\u0026rsquo;s important to note that you may still need to update your Helm chart templates, especially if there have been structural changes between versions. For instance, you might observe differences in the Horizontal Pod Autoscaler (HPA) resource between versions 1.24 and 1.27.\n","permalink":"https://okulbida.com/posts/helm-fix-after-k8s-upgrade/","summary":"After upgrading Kubernetes (k8s), you might encounter errors such as no matches for kind \u0026quot;Deployment\u0026quot; in version \u0026quot;apps/v1beta1\u0026quot;. These errors typically indicate that certain resources have become deprecated. To resolve these issues without the need to delete your Helm chart, you can follow this simple solution:\nhelm plugin install https://github.com/helm/helm-mapkubeapis helm mapkubeapis \u0026lt;releasename\u0026gt; helm upgrade \u0026lt;releasename\u0026gt; It\u0026rsquo;s important to note that you may still need to update your Helm chart templates, especially if there have been structural changes between versions.","title":"Resolving Helm issues after kubernetes upgrade"},{"content":"Expose Amazon EKS pods through cross-account load balancer\n https://aws.amazon.com/blogs/containers/expose-amazon-eks-pods-through-cross-account-load-balancer/  ","permalink":"https://okulbida.com/posts/eks-expose-pods-through-cross-account-lb/","summary":"Expose Amazon EKS pods through cross-account load balancer\n https://aws.amazon.com/blogs/containers/expose-amazon-eks-pods-through-cross-account-load-balancer/  ","title":"EKS expose pods through cross-account Load balancer"},{"content":"Simplified EKS access\n https://aws.amazon.com/about-aws/whats-new/2023/12/amazon-eks-controls-iam-cluster-access-management/ https://aws.amazon.com/blogs/containers/a-deep-dive-into-simplified-amazon-eks-access-management-controls/ https://github.com/hashicorp/terraform-provider-aws/issues/34982  ","permalink":"https://okulbida.com/posts/eks-access/","summary":"Simplified EKS access\n https://aws.amazon.com/about-aws/whats-new/2023/12/amazon-eks-controls-iam-cluster-access-management/ https://aws.amazon.com/blogs/containers/a-deep-dive-into-simplified-amazon-eks-access-management-controls/ https://github.com/hashicorp/terraform-provider-aws/issues/34982  ","title":"EKS simplified access"},{"content":"While using Loki with S3 and Dynamodb it\u0026rsquo;s mandatory to add provision_config details as default might affect your budget https://grafana.com/docs/loki/latest/configuration/#provision_config\n[provisioned_write_throughput: \u0026lt;int\u0026gt; | default = 3000] # DynamoDB table default read throughput. # CLI flag: -\u0026lt;prefix\u0026gt;.read-throughput [provisioned_read_throughput: \u0026lt;int\u0026gt; | default = 300] ","permalink":"https://okulbida.com/posts/loki-s3-dynamodb/","summary":"While using Loki with S3 and Dynamodb it\u0026rsquo;s mandatory to add provision_config details as default might affect your budget https://grafana.com/docs/loki/latest/configuration/#provision_config\n[provisioned_write_throughput: \u0026lt;int\u0026gt; | default = 3000] # DynamoDB table default read throughput. # CLI flag: -\u0026lt;prefix\u0026gt;.read-throughput [provisioned_read_throughput: \u0026lt;int\u0026gt; | default = 300] ","title":"Loki S3 dynamodb"},{"content":"","permalink":"https://okulbida.com/posts/bigdata-comparison-cloudproviders/","summary":"","title":"Bigdata comparison within AWS,Azure,GCP"},{"content":"","permalink":"https://okulbida.com/posts/db-cloudproviders/","summary":"","title":"Database comparison within AWS,Azure,GCP"},{"content":"You can now launch NAT Gateways in your VPC without associating an internet gateway to your VPC. Internet Gateway is required to provide internet access to the NAT Gateway. However, some customers use their NAT Gateways with Transit Gateway or virtual private gateway to communicate privately with other VPCs or on-premises environments and thus, do not need an internet gateway attached to their VPCs.\nMore details: https://aws.amazon.com/about-aws/whats-new/2021/06/aws-removes-nat-gateways-dependence-on-internet-gateway-for-private-communications/\n","permalink":"https://okulbida.com/posts/aws-nat-gateway-no-need-igw/","summary":"You can now launch NAT Gateways in your VPC without associating an internet gateway to your VPC. Internet Gateway is required to provide internet access to the NAT Gateway. However, some customers use their NAT Gateways with Transit Gateway or virtual private gateway to communicate privately with other VPCs or on-premises environments and thus, do not need an internet gateway attached to their VPCs.\nMore details: https://aws.amazon.com/about-aws/whats-new/2021/06/aws-removes-nat-gateways-dependence-on-internet-gateway-for-private-communications/","title":"AWS removes NAT Gateway‚Äôs dependence on Internet Gateway for Private communications"},{"content":"What‚Äôs new in Grafana v8.0\n  Grafana includes built-in support for Prometheus Alertmanager. Once you add it as a data source, you can use the Grafana alerting UI to manage silences, contact points as well as notification policies. A drop down option in these pages allows you to switch between Grafana and any configured Alertmanager data sources. https://grafana.com/docs/grafana/latest/datasources/alertmanager/\n  Prometheus metrics browser https://grafana.com/docs/grafana/latest/datasources/prometheus/#metrics-browser\n  More details: https://grafana.com/docs/grafana/latest/whatsnew/whats-new-in-v8-0/\n","permalink":"https://okulbida.com/posts/grafana-8-released/","summary":"What‚Äôs new in Grafana v8.0\n  Grafana includes built-in support for Prometheus Alertmanager. Once you add it as a data source, you can use the Grafana alerting UI to manage silences, contact points as well as notification policies. A drop down option in these pages allows you to switch between Grafana and any configured Alertmanager data sources. https://grafana.com/docs/grafana/latest/datasources/alertmanager/\n  Prometheus metrics browser https://grafana.com/docs/grafana/latest/datasources/prometheus/#metrics-browser\n  More details: https://grafana.com/docs/grafana/latest/whatsnew/whats-new-in-v8-0/","title":"What‚Äôs new in Grafana v8.0"},{"content":"https://aws.amazon.com/about-aws/whats-new/2021/05/aws-load-balancer-controller-version-2-2-available-support-nlb-instance/\n","permalink":"https://okulbida.com/posts/aws-lb-controller-nlb-support/","summary":"https://aws.amazon.com/about-aws/whats-new/2021/05/aws-load-balancer-controller-version-2-2-available-support-nlb-instance/","title":"AWS Load Balancer Controller version 2.2 now available with support for NLB instance targeting"},{"content":"Amazon EC2 Auto Scaling now natively supports Predictive Scaling so you can proactively scale out your Auto Scaling group to be ready for upcoming demand. Predictive Scaling can help you avoid the need to over-provision capacity, resulting in lower EC2 cost, while ensuring your application‚Äôs responsiveness. (Previously, Predictive Scaling was only available via AWS Auto Scaling Plans.)\nhttps://aws.amazon.com/about-aws/whats-new/2021/05/amazon-ec2-auto-scaling-introduces-predictive-scaling-native-scaling-policy/\n","permalink":"https://okulbida.com/posts/aws-ec2-predictive-autoscaling/","summary":"Amazon EC2 Auto Scaling now natively supports Predictive Scaling so you can proactively scale out your Auto Scaling group to be ready for upcoming demand. Predictive Scaling can help you avoid the need to over-provision capacity, resulting in lower EC2 cost, while ensuring your application‚Äôs responsiveness. (Previously, Predictive Scaling was only available via AWS Auto Scaling Plans.)\nhttps://aws.amazon.com/about-aws/whats-new/2021/05/amazon-ec2-auto-scaling-introduces-predictive-scaling-native-scaling-policy/","title":"Amazon EC2 Auto Scaling Introduces Predictive Scaling as a Native Scaling Policy"},{"content":"Amazon Elastic Kubernetes Service (Amazon EKS) now supports using the Amazon EKS console, CLI, and API to install and manage CoreDNS and kube-proxy in addition to existing support for the Amazon VPC CNI networking plugin.\nhttps://aws.amazon.com/about-aws/whats-new/2021/05/eks-add-ons-now-support-coredns-kube-proxy/\nhttps://docs.aws.amazon.com/eks/latest/userguide/eks-add-ons.html\n","permalink":"https://okulbida.com/posts/awsekscoredns/","summary":"Amazon Elastic Kubernetes Service (Amazon EKS) now supports using the Amazon EKS console, CLI, and API to install and manage CoreDNS and kube-proxy in addition to existing support for the Amazon VPC CNI networking plugin.\nhttps://aws.amazon.com/about-aws/whats-new/2021/05/eks-add-ons-now-support-coredns-kube-proxy/\nhttps://docs.aws.amazon.com/eks/latest/userguide/eks-add-ons.html","title":"EKS Add-Ons Now Supports CoreDNS and kube-proxy"},{"content":"Full list of videos from KubeCon 2021 Europe\nhttps://www.youtube.com/playlist?list=PLj6h78yzYM2MqBm19mRz9SYLsw4kfQBrC\n","permalink":"https://okulbida.com/posts/kubecon2021/","summary":"Full list of videos from KubeCon 2021 Europe\nhttps://www.youtube.com/playlist?list=PLj6h78yzYM2MqBm19mRz9SYLsw4kfQBrC","title":"Kubecon2021"},{"content":"AWS CloudFront functions is a nice alternative to Lambda@Edge\nhttps://aws.amazon.com/blogs/aws/introducing-cloudfront-functions-run-your-code-at-the-edge-with-low-latency-at-any-scale/\n","permalink":"https://okulbida.com/posts/aws-cloudfront-functions/","summary":"AWS CloudFront functions is a nice alternative to Lambda@Edge\nhttps://aws.amazon.com/blogs/aws/introducing-cloudfront-functions-run-your-code-at-the-edge-with-low-latency-at-any-scale/","title":"AWS CloudFront functions"},{"content":"Amazon EC2 enables you to replace the root EBS volume for a running instance\nLimitations:  You can\u0026rsquo;t replace the root volume if it is an instance store volume. You can\u0026rsquo;t replace the root volume for metal instances.  More details: https://aws.amazon.com/about-aws/whats-new/2021/04/ec2-enables-replacing-root-volumes-for-quick-restoration-and-troubleshooting/\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-restoring-volume.html#replace-root\n","permalink":"https://okulbida.com/posts/ec2-root-volume-replacing/","summary":"Amazon EC2 enables you to replace the root EBS volume for a running instance\nLimitations:  You can\u0026rsquo;t replace the root volume if it is an instance store volume. You can\u0026rsquo;t replace the root volume for metal instances.  More details: https://aws.amazon.com/about-aws/whats-new/2021/04/ec2-enables-replacing-root-volumes-for-quick-restoration-and-troubleshooting/\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-restoring-volume.html#replace-root","title":"EC2 root volume replacing"},{"content":"https://y0zg.github.io/public-blog/pf-infographic-promql-cheatsheet.pdf\n","permalink":"https://okulbida.com/posts/prom-cheat-sheet/","summary":"https://y0zg.github.io/public-blog/pf-infographic-promql-cheatsheet.pdf","title":"PromQL cheat sheet"},{"content":"How to use AWS Secrets \u0026amp; Configuration Provider with your Kubernetes Secrets Store CSI driver. One more example of kubernetes secrets management among vault, external-secrets and 1password operator üòÖ\nhttps://aws.amazon.com/blogs/security/how-to-use-aws-secrets-configuration-provider-with-kubernetes-secrets-store-csi-driver/\n","permalink":"https://okulbida.com/posts/aws-secrets-csi/","summary":"How to use AWS Secrets \u0026amp; Configuration Provider with your Kubernetes Secrets Store CSI driver. One more example of kubernetes secrets management among vault, external-secrets and 1password operator üòÖ\nhttps://aws.amazon.com/blogs/security/how-to-use-aws-secrets-configuration-provider-with-kubernetes-secrets-store-csi-driver/","title":"AWS Secrets CSI for EKS"},{"content":"Grafana was relicensed to AGPLv3\nhttps://grafana.com/blog/2021/04/20/grafana-loki-tempo-relicensing-to-agplv3/\n","permalink":"https://okulbida.com/posts/grafana-license/","summary":"Grafana was relicensed to AGPLv3\nhttps://grafana.com/blog/2021/04/20/grafana-loki-tempo-relicensing-to-agplv3/","title":"Grafana License"},{"content":"","permalink":"https://okulbida.com/categories/","summary":"","title":""},{"content":"Oleksandr Kulbida is a seasoned cloud engineer with focus on opensource. His passions are around breaking silos between teams and automating everything! Oleksandr follows Unix philosophy and loves adopting infrastructure as code. Feel free to connect on LinkedIn https://www.linkedin.com/in/oleksandrkulbida/ ","permalink":"https://okulbida.com/about/","summary":"about","title":"About"},{"content":"","permalink":"https://okulbida.com/archive/","summary":"archive","title":"Archive"}]